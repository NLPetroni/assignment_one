{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install wandb # colab only\n",
    "import re\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import defaultdict, OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "import config as cfg\n",
    "\n",
    "def download_and_unzip(url, save_dir='.'):\n",
    "  # downloads and unzips url, if not already downloaded\n",
    "  # used for downloading dataset and glove embeddings\n",
    "  import os\n",
    "  from urllib.request import urlopen\n",
    "  from io import BytesIO\n",
    "  from zipfile import ZipFile\n",
    "  fname = url.split('/')[-1][:-4] if save_dir == '.' else save_dir\n",
    "  if fname not in os.listdir():\n",
    "    print(f'downloading and unzipping {fname}...', end=' ')\n",
    "    r = urlopen(url)\n",
    "    zipf = ZipFile(BytesIO(r.read()))\n",
    "    zipf.extractall(path=save_dir)\n",
    "    print(f'completed')\n",
    "  else:\n",
    "    print(f'{fname} already downloaded')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def get_wandbkey():\n",
    "    with open('wandbkey.txt') as f:\n",
    "        return f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove(emb_size=100, number_token=False):\n",
    "  \"\"\"\n",
    "    Download and load glove embeddings. \n",
    "    Parameters:\n",
    "      emb_size: embedding size (50/100/200/300-dimensional vectors).    \n",
    "    Returns tuple (voc, emb) where voc is dict from words to idx (in emb) and emb is (numpy) embedding matrix\n",
    "  \"\"\"\n",
    "  n_tokens = 400000 + 1 # glove vocabulary size + PAD\n",
    "  if emb_size not in (50, 100, 200, 300):\n",
    "    raise ValueError(f'wrong size parameter: {emb_size}')\n",
    "  \n",
    "  if number_token: \n",
    "    n_tokens += 1\n",
    "  download_and_unzip('http://nlp.stanford.edu/data/glove.6B.zip', save_dir='glove')\n",
    "  vocabulary = dict()\n",
    "  embedding_matrix = np.ones((n_tokens, emb_size))\n",
    "\n",
    "  with open(f'glove/glove.6B.{emb_size}d.txt', encoding=\"utf8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embedding_matrix[i] = coefs\n",
    "        vocabulary[word] = i\n",
    "  \n",
    "  # add embedding for and padding and number token\n",
    "  if number_token:\n",
    "    embedding_matrix[n_tokens - 2] = 0\n",
    "    vocabulary['<PAD>'] = n_tokens - 2\n",
    "    digits = list(filter(lambda s: re.fullmatch('\\d+(\\.\\d*)?', s) is not None, vocabulary.keys()))\n",
    "    embedding_matrix[n_tokens - 1] = np.mean(embedding_matrix[[vocabulary[d] for d in digits]], axis=0)\n",
    "    vocabulary['<NUM>'] = n_tokens - 1\n",
    "  else: \n",
    "    embedding_matrix[n_tokens - 1] = 0\n",
    "    vocabulary['<PAD>'] = n_tokens - 1\n",
    "  return vocabulary, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Simple dataset class to use dataloaders (batching) \"\"\"\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$': 0, 'NN': 1, ',': 2, 'RBS': 3, 'FW': 4, 'CC': 5, '#': 6, 'VBD': 7, 'PRP': 8, 'RBR': 9, 'LS': 10, ':': 11, 'VBZ': 12, 'MD': 13, 'EX': 14, 'RB': 15, 'WRB': 16, 'NNS': 17, 'VBG': 18, 'PRP$': 19, 'JJR': 20, 'WP$': 21, 'WP': 22, '-LRB-': 23, 'WDT': 24, '``': 25, '.': 26, 'CD': 27, 'JJ': 28, \"''\": 29, '<PAD>': 30, 'UH': 31, 'VBN': 32, 'IN': 33, 'SYM': 34, 'DT': 35, 'JJS': 36, '-RRB-': 37, 'RP': 38, 'VB': 39, 'POS': 40, 'NNP': 41, 'PDT': 42, 'NNPS': 43, 'VBP': 44, 'TO': 45}\n"
     ]
    }
   ],
   "source": [
    "def load_classes():\n",
    "    download_and_unzip('https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip')\n",
    "    c = set()\n",
    "    for doc in range(1, 200):\n",
    "        with open(f'dependency_treebank/wsj_{doc:04d}.dp') as f:\n",
    "            for line in f:\n",
    "                if line.strip(): # check for empty lines\n",
    "                    _, label, _ = line.split('\\t')\n",
    "                    #print(label)\n",
    "                    c.add(label)\n",
    "    return c\n",
    "\n",
    "classes = {'$', 'NN', ',', 'RBS', 'FW', 'CC', '#', 'VBD', 'PRP', 'RBR', 'LS', ':', 'VBZ', 'MD',\n",
    "           'EX', 'RB', 'WRB', 'NNS', 'VBG', 'PRP$', 'JJR', 'WP$', 'WP', '-LRB-', 'WDT', '``',\n",
    "           '.', 'CD', 'JJ', \"''\", 'UH', 'VBN', 'IN', 'SYM', 'DT', 'JJS', '-RRB-', 'RP', 'VB',\n",
    "           'POS', 'NNP', 'PDT', 'NNPS', 'VBP', 'TO', '<PAD>'}\n",
    "punctuation_cls = {'$', ',', '#', ':', '-LRB-', '``', '.', \"''\", 'SYM', '-RRB-', '<PAD>'}\n",
    "class2idx = {c: i for i, c in enumerate(classes)}\n",
    "print(class2idx)\n",
    "\n",
    "def add_oov(start_voc, oovs, embedding_matrix, sentences):\n",
    "  \"\"\"\n",
    "    Computes new embedding matrix, adding embeddings for oovs\n",
    "    Parameters:\n",
    "      start_voc: dict, starting vocabulary that is extended with oovs\n",
    "      oovs: set of string, oovs to add to the starting vocabulary and embedding matrix\n",
    "      embedding_matrix: starting embedding matrix (numpy)\n",
    "      sentences: list of list of strings, set used to compute oov embeddings\n",
    "    Returns tuple (voc, emb) where voc is dict from words to idx (in emb) and emb is (numpy) embedding matrix with oovs\n",
    "  \"\"\"\n",
    "  oovs = oovs - set(start_voc.keys())\n",
    "  start_voc_size, emb_size = embedding_matrix.shape\n",
    "  oov_embeddings = np.zeros((start_voc_size + len(oovs), emb_size))\n",
    "  oov_embeddings[:start_voc_size] = embedding_matrix\n",
    "  new_voc = dict(start_voc)\n",
    "\n",
    "  for i, oov in enumerate(oovs):\n",
    "    context_words = [new_voc[word] \n",
    "                    for sentence in filter(lambda s: oov in s, sentences)\n",
    "                    for word in sentence if word in new_voc and word not in (oov, '<PAD>')]\n",
    "    if len(context_words) == 0:\n",
    "        print(f'Empty context for oov: {oov}')\n",
    "    oov_embeddings[start_voc_size + i] = np.mean(oov_embeddings[context_words], axis=0)\n",
    "    new_voc[oov] = start_voc_size + i\n",
    "  return new_voc, oov_embeddings\n",
    "    \n",
    "def load_data(start, end, start_voc, embedding_matrix, number_token=False,\n",
    "              drop_punctuation=True, split_docs=True, ret_counts=False):\n",
    "  \"\"\"\n",
    "    Downloads dataset and preprocess data.\n",
    "    Params:\n",
    "      start: idx of first file to include in data\n",
    "      end: idx of last file to include in data\n",
    "      start_voc: starting vocabulary that is extended with oov terms\n",
    "      embedding_matrix: embedding matrix that \n",
    "      #TODO implement number_token: if True, use a single token for all cardinal numbers\n",
    "      drop_punctuation: if True, drop punt\n",
    "      split_docs: if True, each sequence is one sentence; if false, each sequence is one document\n",
    "      ret_counts: if True, also return counts of each word in the documents\n",
    "    Returns \n",
    "  \"\"\"\n",
    "  # download dataset\n",
    "  download_and_unzip('https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip')\n",
    "\n",
    "  inputs, labels = [], []\n",
    "  vocabulary = set()\n",
    "  counts = defaultdict(int)\n",
    "  \n",
    "  # build dataset\n",
    "  for doc in range(start, end+1):\n",
    "    with open(f'dependency_treebank/wsj_{doc:04d}.dp') as f:\n",
    "      \n",
    "      input_seq, label_seq = [], []\n",
    "      \n",
    "      for line in f:\n",
    "        if line.strip(): # check for empty lines\n",
    "          word, label, _ = line.split('\\t')\n",
    "          word = word.lower()\n",
    "          if '\\/' in word:\n",
    "            word = word.replace('\\/', '-')\n",
    "          if number_token and re.fullmatch('\\d+(\\.\\d*)?', word) is not None:\n",
    "            word = '<NUM>'\n",
    "          if not drop_punctuation or label.isalpha(): # eventually drop punctuation\n",
    "            vocabulary.add(word)\n",
    "            input_seq.append(word)\n",
    "            label_seq.append(label)\n",
    "            counts[word] += 1\n",
    "        elif split_docs: # sentence over, add to input if splitting documents\n",
    "          inputs.append(input_seq)\n",
    "          labels.append(label_seq)\n",
    "          input_seq, label_seq = [], []\n",
    "\n",
    "      inputs.append(input_seq)\n",
    "      labels.append(label_seq)\n",
    "  \n",
    "  max_seq_len = int(np.quantile([len(seq) for seq in inputs], 0.8))\n",
    "  print(max_seq_len)\n",
    "  inputs_copy = []\n",
    "  labels_copy = []\n",
    "  for i_seq, l_seq in zip(inputs, labels):\n",
    "    if len(i_seq) > max_seq_len:\n",
    "        inputs_copy.append(i_seq[:max_seq_len])\n",
    "        labels_copy.append(l_seq[:max_seq_len])\n",
    "    else:\n",
    "        inputs_copy.append(i_seq + ['<PAD>'] * (max_seq_len - len(i_seq)))\n",
    "        labels_copy.append(l_seq + ['<PAD>'] * (max_seq_len - len(l_seq)))\n",
    "\n",
    "  inputs = inputs_copy\n",
    "  labels = labels_copy\n",
    "  vocabulary, embedding_matrix = add_oov(start_voc, vocabulary, embedding_matrix, inputs)\n",
    "  inputs = torch.as_tensor([[vocabulary[word] for word in sequence] for sequence in inputs])\n",
    "  labels = torch.as_tensor([[class2idx[label] for label in sequence] for sequence in labels])\n",
    "\n",
    "  if ret_counts:\n",
    "    return inputs, labels, vocabulary, embedding_matrix, counts\n",
    "  else:\n",
    "    return inputs, labels, vocabulary, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "33\n",
      "Empty context for oov: anku\n",
      "Empty context for oov: 520-lawyer\n",
      "Empty context for oov: 40-megabyte\n",
      "Empty context for oov: stirlen\n",
      "Empty context for oov: pathlogy\n",
      "Empty context for oov: trockenbeerenauslesen\n",
      "Empty context for oov: norwick\n",
      "Empty context for oov: airline-related\n",
      "Empty context for oov: 618.1\n",
      "Empty context for oov: meinders\n",
      "Empty context for oov: jalaalwalikraam\n",
      "Empty context for oov: louisiana-pacific\n",
      "Empty context for oov: chafic\n",
      "Empty context for oov: beer-belly\n",
      "Empty context for oov: futures-related\n",
      "Empty context for oov: flim-flammery\n",
      "Empty context for oov: government-certified\n",
      "Empty context for oov: severable\n",
      "Empty context for oov: certin\n",
      "Empty context for oov: 16,072\n",
      "Empty context for oov: 18,444\n",
      "Empty context for oov: ednie\n",
      "Empty context for oov: 2691.19\n",
      "Empty context for oov: wheeland\n",
      "Empty context for oov: bald-faced\n",
      "Empty context for oov: monchecourt\n",
      "Empty context for oov: truth-in-lending\n",
      "Empty context for oov: glenham\n",
      "Empty context for oov: product-design\n",
      "Empty context for oov: cleaner-burning\n",
      "Empty context for oov: mouth-up\n",
      "Empty context for oov: corporate-wide\n",
      "Empty context for oov: 12,252\n",
      "Empty context for oov: 82,389\n",
      "Empty context for oov: express-buick\n",
      "Empty context for oov: 14,821\n",
      "Empty context for oov: cotran\n",
      "Empty context for oov: 630.9\n",
      "Empty context for oov: asset-valuation\n",
      "Empty context for oov: muscolina\n",
      "Empty context for oov: school-district\n",
      "Empty context for oov: boorse\n",
      "Empty context for oov: car-care\n",
      "tensor(0.3195)\n"
     ]
    }
   ],
   "source": [
    "glove_voc, embedding_matrix = get_glove(number_token=False)\n",
    "split = 'valid'\n",
    "train_set, train_labels, train_voc, embedding_matrix = load_data(1, 199, glove_voc, embedding_matrix, number_token=False, drop_punctuation=False)\n",
    "train_dl = torch.utils.data.DataLoader(Dataset(train_set, train_labels), batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "print((train_labels == class2idx['<PAD>']).sum() / np.prod(train_labels.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, embedding_matrix, type, rec_size=1, units=None, hid_size=50):\n",
    "    \"\"\"\n",
    "      A recurrent network performing multiclass classification (POS tagging).\n",
    "      Params:\n",
    "        type: type of rnn, either 'lstm' or 'gru'\n",
    "        embedding_matrix: embedding matrix for embedding layer\n",
    "        rec_size: number of stacked recurrent modules\n",
    "        units: int or None, if given then add one additional linear layer with given number of units\n",
    "        hid_size: size of hidden state of recurrent module\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    emb_size = embedding_matrix.shape[1]\n",
    "    self.emb_layer = nn.Embedding.from_pretrained(torch.as_tensor(embedding_matrix))\n",
    "\n",
    "    if type == 'lstm':\n",
    "      rec_module = nn.LSTM\n",
    "    elif type == 'gru':\n",
    "      rec_module = nn.GRU\n",
    "    else:\n",
    "      raise ValueError(f'wrong type {type}, either lstm or gru')\n",
    "    self.rec_modules = rec_module(input_size=emb_size, hidden_size=hid_size, bidirectional=True, batch_first=True, num_layers=rec_size)\n",
    "    cls = len(classes)\n",
    "    fc_params = [2 * hid_size] + ([units, cls] if units is not None else [cls])\n",
    "    self.fc_modules = nn.Sequential(\n",
    "      OrderedDict([(f'fc_{i}', nn.Linear(in_shape, out_shape)) \n",
    "      for i, (in_shape, out_shape) in enumerate(zip(fc_params[:-1], fc_params[1:]))]))\n",
    "      \n",
    "    # self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    vecs = self.emb_layer(x).float()\n",
    "    rec_out, _ = self.rec_modules(vecs)\n",
    "    fc_out = self.fc_modules(rec_out)\n",
    "    return fc_out\n",
    "    # return self.logsoftmax(fc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loss_fn, data_loader, device):\n",
    "    \"\"\" \n",
    "        Trains model for one epoch on the given dataloader.\n",
    "        Parameters:\n",
    "            model: torch.nn.Module to train\n",
    "            optimizer: torch.optim optimizer object\n",
    "            loss_fn: torch.nn criterion to use to compute loss, given outputs and targets\n",
    "            data_loader: torch.utils.data.DataLoader \n",
    "            device: torch.device where training is performed\n",
    "        Returns log dict {'train/loss' : list(loss values for each batch)} \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    log_dict = {'train/loss': []}\n",
    "\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logprobs = model(inputs).transpose(1, 2)\n",
    "        loss = loss_fn(logprobs, targets)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log_dict['train/loss'].append(loss_value)\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "def evaluate(model, loss_fn, data_loader, device, split, ret_f1_classes=False):\n",
    "    \"\"\" \n",
    "        Evaluate model on the given dataloader.\n",
    "        Parameters:\n",
    "            model: torch.nn.Module to evaluate\n",
    "            loss_fn: torch.nn criterion to use to compute loss, given outputs and targets\n",
    "            data_loader: torch.utils.data.DataLoader \n",
    "            device: torch.device where evaluation is performed\n",
    "            split: either 'valid' or 'test'\n",
    "            ret_f1_classes: if True, also returns per-class f1 scores\n",
    "        Returns log dict {'valid/loss' : mean loss, 'valid/{metric}': mean metric} \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    assert len(data_loader) == 1 # must be a single batch\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = next(iter(data_loader))\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logprobs = model(inputs).transpose(1, 2)\n",
    "        losses = loss_fn(logprobs, targets).item()\n",
    "        preds = torch.argmax(logprobs, 1)\n",
    "\n",
    "        #print(targets.shape)\n",
    "        mask = [targets.cpu().numpy() != class2idx[c] for c in punctuation_cls]\n",
    "        #print(len(mask), mask[0].shape)\n",
    "        mask = np.array(reduce(lambda a,b: a & b, mask)).reshape(targets.shape)\n",
    "        #print(mask.shape, mask)\n",
    "        #targets = np.where(mask, targets, -1)\n",
    "        #preds = np.where(mask, preds, -1)\n",
    "        #print(targets.shape)\n",
    "        acc = ((np.where(mask, targets==preds, False)).sum() / mask.sum()).item()\n",
    "        f1_classes = f1_score(targets.cpu().numpy().reshape(-1), preds.cpu().numpy().reshape(-1),\n",
    "                      labels=[class2idx[c] for c in (classes - punctuation_cls)], average=None, zero_division=1)\n",
    "\n",
    "    log_dict = {f'{split}/loss': np.mean(losses),\n",
    "                f'{split}/accuracy': np.mean(acc),\n",
    "                f'{split}/f1': np.mean(f1_classes)}\n",
    "    if ret_f1_classes:\n",
    "        return log_dict, f1_classes\n",
    "    else:\n",
    "        return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(verbose=False, test=False, number_token=False, weighted_loss=False):\n",
    "    \"\"\" Fully trains one model, based on cfg parameters, on training set and performs evaluation on validation set.\n",
    "        Returns trained model.\n",
    "    \"\"\"\n",
    "    cfg_dict = {\n",
    "        'epochs': cfg.EPOCHS, 'batch_size': cfg.BATCH_SIZE, 'number_token': number_token,\n",
    "        'model': cfg.TYPE, 'rec_size': cfg.REC_SIZE, 'units': cfg.UNITS, 'hid_size': cfg.HID_SIZE,\n",
    "        'optim': cfg.OPTIM, 'lr': cfg.LR, 'alpha': cfg.ALPHA, 'betas': cfg.BETAS, 'momentum': cfg.MOMENTUM, 'weight_decay': cfg.WEIGHT_DECAY\n",
    "    }\n",
    "    if verbose:\n",
    "        print('CONFIG PARAMETERS:')\n",
    "        pprint(cfg_dict)\n",
    "    metric = 'f1' if test else 'accuracy'\n",
    "    wandb.login(key=get_wandbkey())\n",
    "    run = wandb.init(project=\"assignment-one\", entity=\"nlpetroni\", group=f'{\"testing\" if test else \"validation\"}', reinit=True, config=cfg_dict)\n",
    "    wandb.define_metric(\"train_step\")\n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric('train/loss', step_metric=\"train_step\", summary=\"min\")\n",
    "    wandb.define_metric(f\"valid/loss\", step_metric=\"epoch\", summary=\"min\")\n",
    "    wandb.define_metric(f\"valid/accuracy\", step_metric=\"epoch\", summary=\"max\")\n",
    "    wandb.define_metric(f\"valid/f1\", step_metric=\"epoch\", summary=\"max\")\n",
    "    wandb.define_metric(f\"test/accuracy\", step_metric=\"epoch\", summary=\"max\")\n",
    "    wandb.define_metric(f\"test/f1\", step_metric=\"epoch\", summary=\"max\")\n",
    "\n",
    "    glove_voc, embedding_matrix = get_glove(number_token=number_token)\n",
    "    if not test:\n",
    "        split = 'valid'\n",
    "        train_set, train_labels, train_voc, embedding_matrix = load_data(1, 100, glove_voc, embedding_matrix, number_token=number_token, drop_punctuation=False)\n",
    "        valid_set, valid_labels, valid_voc, embedding_matrix = load_data(101, 150, train_voc, embedding_matrix, number_token=number_token, drop_punctuation=False)\n",
    "        train_dl = torch.utils.data.DataLoader(Dataset(train_set, train_labels), batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "        valid_dl = torch.utils.data.DataLoader(Dataset(valid_set, valid_labels), batch_size=valid_set.shape[0])\n",
    "    else:\n",
    "        split = 'test'\n",
    "        train_set, train_labels, train_voc, embedding_matrix = load_data(1, 150, glove_voc, embedding_matrix, number_token=number_token, drop_punctuation=False)\n",
    "        test_set, test_labels, test_voc, embedding_matrix = load_data(151, 200, train_voc, embedding_matrix, number_token=number_token, drop_punctuation=False)\n",
    "        train_dl = torch.utils.data.DataLoader(Dataset(train_set, train_labels), batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "        test_dl = torch.utils.data.DataLoader(Dataset(test_set, test_labels), batch_size=test_set.shape[0])\n",
    "\n",
    "\n",
    "    model = POSTagger(embedding_matrix, type=cfg.TYPE, rec_size=cfg.REC_SIZE, units=cfg.UNITS, hid_size=cfg.HID_SIZE).to(device)\n",
    "    wandb.watch(model, log_graph=True)\n",
    "    if verbose:\n",
    "        print(summary(model))\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if cfg.OPTIM == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(params, lr=cfg.LR, alpha=cfg.ALPHA, momentum=cfg.MOMENTUM, weight_decay=cfg.WEIGHT_DECAY)\n",
    "    elif cfg.OPTIM == 'adam':\n",
    "        optimizer = torch.optim.Adam(params, lr=cfg.LR, betas=cfg.BETAS, weight_decay=cfg.WEIGHT_DECAY)\n",
    "    else:\n",
    "        raise ValueError(f'wrong optim {cfg.OPTIM}, either rmsprop or adam')\n",
    "    if weighted_loss:\n",
    "        _, counts = np.unique(valid_labels, return_counts=True)\n",
    "        weight = np.where(counts < 100, 1, 1)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=torch.as_tensor(weight), ignore_index=class2idx['<PAD>'])\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=class2idx['<PAD>']) # ignore padding\n",
    "    train_step = 0\n",
    "\n",
    "    print('STARTING TRAINING')\n",
    "    for epoch in range(cfg.EPOCHS):\n",
    "        log_dict = train_one_epoch(model, optimizer, loss_fn, train_dl, device)\n",
    "        if not test:\n",
    "            log_dict.update(evaluate(model, loss_fn, valid_dl, device, split=split, metric=metric, ret_f1_classes=True))\n",
    "        for batch_loss in log_dict['train/loss']:\n",
    "            wandb.log({'train_step': train_step, 'epoch': epoch, 'train/loss': batch_loss})\n",
    "            train_step += 1\n",
    "        wandb.log({'epoch': epoch, 'valid/loss': log_dict['valid/loss'], 'valid/accuracy': log_dict['valid/accuracy']})\n",
    "        if (epoch % 25) == 0:\n",
    "            print(f'[{epoch:03d}/{cfg.EPOCHS:03d}] train loss: {np.mean(log_dict[\"train/loss\"]):.3f}, valid loss: {log_dict[\"valid/loss\"]:.3f}, accuracy: {log_dict[\"valid/accuracy\"]:.2f}')\n",
    "    if test:\n",
    "        log_dict = evaluate(model, loss_fn, test_dl, device, split=split, metric=metric, ret_f1_classes=True)\n",
    "        wandb.log(log_dict)\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "    return model, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:29cle0zq) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2224... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\">\n</div><div class=\"wandb-col\">\n</div></div>\nSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n<br/>Synced <strong style=\"color:#cdcd00\">lyric-waterfall-693</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/29cle0zq\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/29cle0zq</a><br/>\nFind logs at: <code>.\\wandb\\run-20211205_180145-29cle0zq\\logs</code><br/>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:29cle0zq). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/24lbrq85\" target=\"_blank\">clean-sound-694</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "['mr.', 'vinken', 'is', 'chairman', 'of', 'elsevier', 'n.v.', ',', 'the', 'dutch', 'publishing', 'group', '.']\n",
      "['rudolph', 'agnew', ',', '55', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', ',', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', '.']\n",
      "['it', 'has', 'no', 'bearing', 'on', 'our', 'work', 'force', 'today', '.']\n",
      "['the', '30-day', 'simple', 'yield', 'fell', 'to', 'an', 'average', '8.19', '%', 'from', '8.22', '%', ';', 'the', '30-day', 'compound', 'yield', 'slid', 'to', 'an', 'average', '8.53', '%', 'from', '8.56', '%', '.']\n",
      "['w.r.', 'grace', 'holds', 'three', 'of', 'grace', 'energy', \"'s\", 'seven', 'board', 'seats', '.']\n",
      "['the', 'thrift', 'holding', 'company', 'said', 'it', 'expects', 'to', 'obtain', 'regulatory', 'approval', 'and', 'complete', 'the', 'transaction', 'by', 'year-end', '.']\n",
      "['it', 'employs', '2,700', 'people', 'and', 'has', 'annual', 'revenue', 'of', 'about', '$', '370', 'million', '.']\n",
      "['the', 'treasury', 'said', 'the', 'u.s.', 'will', 'default', 'on', 'nov.', '9', 'if', 'congress', 'does', \"n't\", 'act', 'by', 'then', '.']\n",
      "['he', 'had', 'been', 'a', 'sales', 'and', 'marketing', 'executive', 'with', 'chrysler', 'for', '20', 'years', '.']\n",
      "['but', 'for', 'now', ',', 'they', \"'re\", 'looking', 'forward', 'to', 'their', 'winter', 'meeting', '--', 'boca', 'in', 'february', '.']\n",
      "['imports', 'were', 'at', '$', '50.38', 'billion', ',', 'up', '19', '%', '.']\n",
      "['u.s.', 'news', \"'\", 'circulation', 'in', 'the', 'same', 'time', 'was', '2,303,328', ',', 'down', '2.6', '%', '.']\n",
      "['ps', 'of', 'new', 'hampshire', 'shares', 'closed', 'yesterday', 'at', '$', '3.75', ',', 'off', '25', 'cents', ',', 'in', 'new', 'york', 'stock', 'exchange', 'composite', 'trading', '.']\n",
      "['they', 'succeed', 'daniel', 'm.', 'rexinger', ',', 'retired', 'circuit', 'city', 'executive', 'vice', 'president', ',', 'and', 'robert', 'r.', 'glauber', ',', 'u.s.', 'treasury', 'undersecretary', ',', 'on', 'the', '12-member', 'board', '.']\n",
      "['last', 'year', 'commonwealth', 'edison', 'had', 'to', 'refund', '$', '72.7', 'million', 'for', 'poor', 'performance', 'of', 'its', 'lasalle', 'i', 'nuclear', 'plant', '.']\n",
      "['sales', 'of', 'medium-sized', 'cars', ',', 'which', 'benefited', 'from', 'price', 'reductions', 'arising', 'from', 'introduction', 'of', 'the', 'consumption', 'tax', ',', 'more', 'than', 'doubled', 'to', '30,841', 'units', 'from', '13,056', 'in', 'october', '1988', '.']\n",
      "['the', 'plant', 'will', 'produce', 'control', 'devices', 'used', 'in', 'motor', 'vehicles', 'and', 'household', 'appliances', '.']\n",
      "['cray', 'computer', ',', 'which', 'currently', 'employs', '241', 'people', ',', 'said', 'it', 'expects', 'a', 'work', 'force', 'of', '450', 'by', 'the', 'end', 'of', '1990', '.']\n",
      "['he', 'was', 'previously', 'vice', 'president', '.']\n",
      "['so', 'far', ',', 'mrs.', 'hills', 'has', \"n't\", 'deemed', 'any', 'cases', 'bad', 'enough', 'to', 'merit', 'an', 'accelerated', 'investigation', 'under', 'the', 'so-called', 'special', '301', 'provision', 'of', 'the', 'act', '.']\n",
      "['but', 'he', 'has', 'not', 'said', 'before', 'that', 'the', 'country', 'wants', 'half', 'the', 'debt', 'forgiven', '.']\n",
      "['today', ',', 'pc', 'shipments', 'annually', 'total', 'some', '$', '38.3', 'billion', 'world-wide', '.']\n",
      "['that', 'stake', ',', 'together', 'with', 'its', 'convertible', 'preferred', 'stock', 'holdings', ',', 'gives', 'faulding', 'the', 'right', 'to', 'increase', 'its', 'interest', 'to', '70', '%', 'of', 'moleculon', \"'s\", 'voting', 'stock', '.']\n",
      "['esso', 'said', 'the', 'fields', 'were', 'developed', 'after', 'the', 'australian', 'government', 'decided', 'in', '1987', 'to', 'make', 'the', 'first', '30', 'million', 'barrels', 'from', 'new', 'fields', 'free', 'of', 'excise', 'tax', '.']\n",
      "['the', 'sale', 'of', 'southern', 'optical', 'is', 'a', 'part', 'of', 'the', 'program', '.']\n",
      "['imports', 'of', 'the', 'types', 'of', 'watches', 'that', 'now', 'will', 'be', 'eligible', 'for', 'duty-free', 'treatment', 'totaled', 'about', '$', '37.3', 'million', 'in', '1988', ',', 'a', 'relatively', 'small', 'share', 'of', 'the', '$', '1.5', 'billion', 'in', 'u.s.', 'watch', 'imports', 'that', 'year', ',', 'according', 'to', 'an', 'aide', 'to', 'u.s.', 'trade', 'representative', 'carla', 'hills', '.']\n",
      "['magna', 'said', 'mr.', 'mcalpine', 'resigned', 'to', 'pursue', 'a', 'consulting', 'career', ',', 'with', 'magna', 'as', 'one', 'of', 'his', 'clients', '.']\n",
      "['lord', 'chilver', ',', '63-year-old', 'chairman', 'of', 'english', 'china', 'clays', 'plc', ',', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'chemical', 'company', '.']\n",
      "['they', 'also', 'have', 'become', 'large', 'purchasers', 'of', 'fannie', 'mae', \"'s\", 'corporate', 'debt', ',', 'buying', '$', '2.4', 'billion', 'in', 'fannie', 'mae', 'bonds', 'during', 'the', 'first', 'nine', 'months', 'of', 'the', 'year', ',', 'or', 'almost', 'a', 'tenth', 'of', 'the', 'total', 'amount', 'issued', '.']\n",
      "['james', 'l.', 'pate', ',', '54-year-old', 'executive', 'vice', 'president', ',', 'was', 'named', 'a', 'director', 'of', 'this', 'oil', 'concern', ',', 'expanding', 'the', 'board', 'to', '14', 'members', '.']\n",
      "['the', 'company', 'is', 'operating', 'under', 'chapter', '11', 'of', 'the', 'federal', 'bankruptcy', 'code', ',', 'giving', 'it', 'court', 'protection', 'from', 'creditors', \"'\", 'lawsuits', 'while', 'it', 'attempts', 'to', 'work', 'out', 'a', 'plan', 'to', 'pay', 'its', 'debts', '.']\n",
      "['the', 'offer', 'is', 'being', 'launched', 'pursuant', 'to', 'a', 'previously', 'announced', 'agreement', 'between', 'the', 'companies', '.']\n",
      "['the', 'announcement', 'follows', 'a', 'sharper', '$', '2.2', 'billion', 'decline', 'in', 'the', 'country', \"'s\", 'foreign', 'reserves', 'in', 'september', 'to', '$', '86.12', 'billion', '.']\n",
      "['for', 'people', 'who', 'insist', 'on', 'jumping', 'in', 'now', 'to', 'buy', 'the', 'funds', ',', 'newgate', \"'s\", 'mr.', 'foot', 'says', ':', '``', 'the', 'only', 'advice', 'i', 'have', 'for', 'these', 'folks', 'is', 'that', 'those', 'who', 'come', 'to', 'the', 'party', 'late', 'had', 'better', 'be', 'ready', 'to', 'leave', 'quickly', '.']\n",
      "['but', 'the', 'soviets', 'might', 'still', 'face', 'legal', 'obstacles', 'to', 'raising', 'money', 'in', 'the', 'u.s.', 'until', 'they', 'settle', 'hundreds', 'of', 'millions', 'of', 'dollars', 'in', 'additional', 'debt', 'still', 'outstanding', 'from', 'the', 'world', 'war', 'ii', 'lend-lease', 'program', '.']\n",
      "['here', 'are', 'the', 'commerce', 'department', \"'s\", 'latest', 'figures', 'for', 'manufacturers', 'in', 'billions', 'of', 'dollars', ',', 'seasonally', 'adjusted', '.']\n",
      "['but', 'the', 'number', 'of', 'weddings', 'last', 'year', '--', '271,124', '--', 'was', 'still', 'well', 'below', 'the', '400,000', 'registered', 'in', '1972', ',', 'the', 'last', 'year', 'of', 'increasing', 'marriages', '.']\n",
      "['bramalea', 'said', 'it', 'expects', 'to', 'complete', 'the', 'issue', 'by', 'the', 'end', 'of', 'the', 'month', '.']\n",
      "['video', 'tip', ':', 'before', 'seeing', '``', 'sidewalk', 'stories', ',', \"''\", 'take', 'a', 'look', 'at', '``', 'city', 'lights', ',', \"''\", 'chaplin', \"'s\", 'tramp', 'at', 'his', 'finest', '.']\n",
      "['if', 'boeing', 'goes', 'ahead', 'with', 'the', 'larger', '767', ',', 'the', 'plane', 'could', 'hit', 'the', 'market', 'in', 'the', 'mid-1990s', '.']\n",
      "['in', '1966', ',', 'on', 'route', 'to', 'a', 're-election', 'rout', 'of', 'democrat', 'frank', \"o'connor\", ',', 'gop', 'gov.', 'nelson', 'rockefeller', 'of', 'new', 'york', 'appeared', 'in', 'person', 'saying', ',', '``', 'if', 'you', 'want', 'to', 'keep', 'the', 'crime', 'rates', 'high', ',', \"o'connor\", 'is', 'your', 'man', '.', \"''\"]\n",
      "['the', 'record', 'price', 'for', 'a', 'full', 'membership', 'on', 'the', 'exchange', 'is', '$', '550,000', ',', 'set', 'aug.', '31', ',', '1987', '.']\n",
      "['``', 'they', 'do', \"n't\", 'want', 'japan', 'to', 'monopolize', 'the', 'region', 'and', 'sew', 'it', 'up', ',', \"''\", 'says', 'chong-sik', 'lee', ',', 'professor', 'of', 'east', 'asian', 'politics', 'at', 'the', 'university', 'of', 'pennsylvania', '.']\n",
      "['despite', 'the', 'strong', 'evidence', 'against', 'mrs.', 'yeargin', ',', 'popular', 'sentiment', 'was', 'so', 'strong', 'in', 'her', 'favor', ',', 'mrs.', 'ward', 'says', ',', 'that', '``', 'i', \"'m\", 'afraid', 'a', 'jury', 'would', \"n't\", 'have', 'convicted', 'her', '.']\n",
      "['messrs.', 'brownell', 'and', 'kean', 'say', 'they', 'are', 'unaware', 'of', 'any', 'efforts', 'by', 'mcgraw-hill', 'to', 'modify', 'or', 'discontinue', 'scoring', 'high', '.']\n",
      "['the', 'purchase', 'price', 'includes', 'two', 'ancillary', 'companies', '.']\n",
      "['the', 'nih', 'currently', 'spends', 'about', '$', '8', 'million', 'annually', 'on', 'fetal-tissue', 'research', 'out', 'of', 'a', 'total', 'research', 'budget', 'of', '$', '8', 'billion', '.']\n",
      "['in', 'the', 'year-earlier', 'period', ',', 'sci', 'had', 'net', 'income', 'of', '$', '4.8', 'million', ',', 'or', '23', 'cents', 'a', 'share', ',', 'on', 'revenue', 'of', '$', '225.6', 'million', '.']\n",
      "['mr.', 'bromwich', ',', '35', ',', 'also', 'has', 'served', 'as', 'deputy', 'chief', 'and', 'chief', 'of', 'the', 'narcotics', 'unit', 'for', 'the', 'u.s.', 'attorney', \"'s\", 'office', 'for', 'the', 'southern', 'district', 'of', 'new', 'york', ',', 'based', 'in', 'manhattan', '.']\n",
      "['the', 'tire', 'maker', 'said', 'the', 'buildings', 'consist', 'of', '1.8', 'million', 'square', 'feet', 'of', 'office', ',', 'manufacturing', 'and', 'warehousing', 'space', 'on', '353', 'acres', 'of', 'land', '.']\n",
      "['``', 'profit', 'may', 'be', 'low', ',', 'but', 'at', 'least', 'costs', 'should', 'be', 'covered', '.']\n",
      "['ntg', 'was', 'formed', 'by', 'osborn', 'communications', 'corp.', 'and', 'desai', 'capital', '.']\n",
      "['sales', 'in', 'stores', 'open', 'more', 'than', 'one', 'year', 'rose', '3', '%', 'to', '$', '29.3', 'million', 'from', '$', '28.4', 'million', '.']\n",
      "['furukawa', 'said', 'the', 'purchase', 'of', 'the', 'french', 'and', 'german', 'plants', 'together', 'will', 'total', 'about', '40', 'billion', 'yen', '-lrb-', '$', '280', 'million', '-rrb-', '.']\n",
      "['structural', 'dynamics', 'research', 'corp.', ',', 'which', 'makes', 'computer-aided', 'engineering', 'software', ',', 'said', 'it', 'introduced', 'new', 'technology', 'in', 'mechanical', 'design', 'automation', 'that', 'will', 'improve', 'mechanical', 'engineering', 'productivity', '.']\n",
      "['guaranteed', 'minimum', '6', '%', '.']\n",
      "['part', 'of', 'the', 'problem', 'is', 'that', 'chip', 'buyers', 'are', 'keeping', 'inventories', 'low', 'because', 'of', 'jitters', 'about', 'the', 'course', 'of', 'the', 'u.s.', 'economy', '.']\n",
      "['william', 'g.', 'kuhns', ',', 'former', 'chairman', 'and', 'chief', 'executive', 'officer', 'of', 'general', 'public', 'utilities', 'corp.', ',', 'was', 'elected', 'a', 'director', 'of', 'this', 'maker', 'of', 'industrial', 'and', 'construction', 'equipment', ',', 'increasing', 'board', 'membership', 'to', '10', '.']\n",
      "['in', 'early', 'trading', 'in', 'hong', 'kong', 'thursday', ',', 'gold', 'was', 'quoted', 'at', '$', '374.19', 'an', 'ounce', '.']\n",
      "['but', 'he', 'adds', ',', '``', 'i', 'feel', 'pressured', ',', 'disappointed', ',', 'uncomfortable', 'and', ',', 'frankly', ',', 'quite', 'angry', 'with', 'viacom', '.']\n",
      "['life', 'of', 'georgia', 'is', 'part', 'of', 'the', 'nationale', 'nederlanden', 'group', ',', 'based', 'in', 'the', 'netherlands', '.']\n",
      "['terms', 'were', \"n't\", 'disclosed', '.']\n",
      "['in', 'new', 'york', 'stock', 'exchange', 'composite', 'trading', 'yesterday', ',', 'sea', 'containers', 'closed', 'at', '$', '62.625', ',', 'up', '62.5', 'cents', '.']\n",
      "['mr.', 'leinonen', 'said', 'he', 'expects', 'ford', 'to', 'meet', 'the', 'deadline', 'easily', '.']\n",
      "['this', 'year', ',', 'the', 'railroad', 'holding', 'company', 'acquired', '850', 'such', 'railcars', '.']\n",
      "['sir', 'peter', 'will', 'succeed', 'sir', 'john', 'milne', ',', '65', ',', 'who', 'retires', 'as', 'blue', 'circle', 'nonexecutive', 'chairman', 'on', 'june', '1', '.']\n",
      "['it', 'recently', 'signed', 'a', 'preliminary', 'agreement', 'to', 'negotiate', 'exclusively', 'with', 'the', 'bank', 'of', 'tokyo', 'ltd.', 'for', 'the', 'sale', 'of', 'part', 'of', 'its', 'leasing', 'business', 'to', 'the', 'japanese', 'bank', '.']\n",
      "['the', 'company', 'also', 'adopted', 'an', 'anti-takeover', 'plan', '.']\n",
      "['mr.', 'butler', 'will', 'remain', 'on', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', '.']\n",
      "['giant', 'group', 'is', 'led', 'by', 'three', 'rally', \"'s\", 'directors', ',', 'burt', 'sugarman', ',', 'james', 'm.', 'trotter', 'iii', 'and', 'william', 'e.', 'trotter', 'ii', ',', 'who', 'last', 'month', 'indicated', 'they', 'hold', 'a', '42.5', '%', 'stake', 'in', 'rally', \"'s\", 'and', 'plan', 'to', 'seek', 'a', 'majority', 'of', 'seats', 'on', 'rally', \"'s\", 'nine-member', 'board', '.']\n",
      "['ms.', 'ensrud', 'is', 'a', 'free-lance', 'wine', 'writer', 'in', 'new', 'york', '.']\n",
      "['``', 'there', 'is', 'always', 'a', 'chance', 'of', 'recession', ',', \"''\", 'added', 'mr.', 'guffey', ',', '``', 'but', 'if', 'you', 'ask', 'me', 'to', 'put', 'a', 'percentage', 'on', 'it', ',', 'i', 'would', 'think', 'it', \"'s\", 'well', 'below', 'a', '50', '%', 'chance', '.']\n",
      "['hallwood', ',', 'a', 'cleveland', 'merchant', 'bank', ',', 'owns', 'about', '11', '%', 'of', 'integra', '.']\n",
      "['it', 'said', 'it', 'has', 'taken', 'measures', 'to', 'continue', 'shipments', 'during', 'the', 'work', 'stoppage', '.']\n",
      "['in', 'the', 'first', 'three', 'months', 'of', '1990', ',', 'the', 'treasury', 'estimates', 'that', 'it', 'will', 'have', 'to', 'raise', 'between', '$', '45', 'billion', 'and', '$', '50', 'billion', ',', 'assuming', 'that', 'it', 'decides', 'to', 'aim', 'for', 'a', '$', '10', 'billion', 'cash', 'balance', 'at', 'the', 'end', 'of', 'march', '.']\n",
      "['reames', ',', 'a', 'maker', 'and', 'marketer', 'of', 'frozen', 'noodles', 'and', 'pre-cooked', 'pasta', 'based', 'in', 'clive', ',', 'iowa', ',', 'has', 'annual', 'sales', 'of', 'about', '$', '11', 'million', ',', 'lancaster', 'said', '.']\n",
      "['in', 'late', 'afternoon', 'new', 'york', 'trading', 'the', 'currency', 'was', 'at', '1.8500', 'marks', 'and', '143.80', 'yen', 'compared', 'with', '1.8415', 'marks', 'and', '142.85', 'yen', '.']\n",
      "['net', 'income', 'more', 'than', 'tripled', 'to', '4.898', 'billion', 'yen', 'from', '1.457', 'billion', 'yen', 'a', 'year', 'earlier', '.']\n",
      "['eaton', 'is', 'an', 'automotive', 'parts', ',', 'controls', 'and', 'aerospace', 'electronics', 'concern', '.']\n",
      "['sales', 'fell', 'to', '$', '251.2', 'million', 'from', '$', '278.7', 'million', '.']\n",
      "['in', 'new', 'brunswick', ',', 'n.j.', ',', 'a', 'johnson', '&', 'johnson', 'spokesman', 'declined', 'comment', '.']\n",
      "['elisabeth', 'rubinfien', 'contributed', 'to', 'this', 'article', '.']\n",
      "['a', '1', '%', 'to', '2', '%', 'return', 'on', '$', '17', 'billion', '``', 'ai', \"n't\", 'hay', ',', \"''\", 'mr.', 'klauser', 'says', '.']\n",
      "['his', 'duties', 'as', 'chief', 'executive', 'will', 'be', 'assumed', 'by', 'chairman', 'jay', 'b.', 'langner', '.']\n",
      "['iras', '.']\n",
      "['sharedata', 'develops', 'and', 'markets', 'low-cost', 'software', ',', 'peripheral', 'equipment', 'and', 'accessories', 'for', 'computers', '.']\n",
      "['mr.', 'baris', 'is', 'a', 'lawyer', 'in', 'new', 'york', '.']\n",
      "['the', 'futures', 'industry', 'is', 'regulated', 'by', 'the', 'commodity', 'futures', 'trading', 'commission', ',', 'which', 'reports', 'to', 'the', 'agriculture', 'committees', 'in', 'both', 'houses', '.']\n",
      "['i', 'have', 'seen', 'one', 'or', 'two', 'men', 'die', ',', 'bless', 'them', '.']\n",
      "['with', 'slower', 'economic', 'growth', 'and', 'flat', 'corporate', 'earnings', 'likely', 'next', 'year', ',', '``', 'i', 'would', \"n't\", 'look', 'for', 'the', 'market', 'to', 'have', 'much', 'upside', 'from', 'current', 'levels', '.']\n",
      "['oliver', 'berliner', 'beverly', 'hills', ',', 'calif', '.']\n",
      "['in', 'new', 'york', 'stock', 'exchange', 'composite', 'trading', 'yesterday', ',', 'nbi', 'common', 'closed', 'at', '93', 'cents', 'a', 'share', ',', 'up', '31', 'cents', '.']\n",
      "['the', 'rifles', 'were', \"n't\", 'loaded', '.']\n",
      "['hiroshi', 'asada']\n",
      "['preston', 'g.', 'foster', 'birmingham', ',', 'ala', '.']\n",
      "['it', 'was', 'just', 'a', 'stupid', 'mistake', 'to', 'get', 'the', 'license', ',', \"''\", 'he', 'said', ',', 'adding', ',', '``', 'i', \"'d\", 'just', 'as', 'soon', 'not', 'get', 'into', \"''\", 'details', 'of', 'the', 'settlement', '.']\n",
      "['this', 'compares', 'with', 'estimates', 'that', 'the', 'u.s.', '``', 'derivatives', \"''\", 'market', 'is', 'perhaps', 'four', 'times', 'as', 'large', 'as', 'the', 'underlying', 'domestic', 'market', '.']\n",
      "['the', 'u.s.', 'chamber', 'of', 'commerce', ',', 'still', 'opposed', 'to', 'any', 'mininum-wage', 'increase', ',', 'said', 'the', 'compromise', 'plan', 'to', 'lift', 'the', 'wage', 'floor', '27', '%', 'in', 'two', 'stages', 'between', 'april', '1990', 'and', 'april', '1991', '``', 'will', 'be', 'impossible', 'for', 'many', 'employers', 'to', 'accommodate', 'and', 'will', 'result', 'in', 'the', 'elimination', 'of', 'jobs', 'for', 'american', 'workers', 'and', 'higher', 'prices', 'for', 'american', 'consumers', '.']\n",
      "['federal', 'data', 'corp.', 'got', 'a', '$', '29.4', 'million', 'air', 'force', 'contract', 'for', 'intelligence', 'data', 'handling', '.']\n",
      "['says', 'kathryn', 'mcauley', ',', 'an', 'analyst', 'at', 'first', 'manhattan', 'co.', ':', '``', 'this', 'is', 'the', 'greatest', 'acquisition', 'challenge', 'he', 'has', 'faced', '.']\n",
      "dependency_treebank already downloaded\n",
      "['and', 'after', 'losing', 'a', 'battle', 'tuesday', 'night', 'with', 'the', 'senate', 'foreign', 'relations', 'committee', ',', 'appropriators', 'from', 'both', 'houses', 'are', 'expected', 'to', 'be', 'forced', 'back', 'to', 'conference', '.']\n",
      "['a', 'spokesman', 'for', 'the', 'state', ',', 'however', ',', 'calls', 'the', 'idea', '``', 'not', 'effective', 'or', 'cost', 'efficient', '.']\n",
      "['the', 'company', 'is', 'contesting', 'the', 'fine', '.']\n",
      "['it', 'should', 'be', 'the', 'natural', 'resources', 'defense', 'council', '.']\n",
      "['ruth', 'k.', 'nelson', 'cullowhee', ',', 'n.c', '.']\n",
      "['he', 'declined', 'to', 'discuss', 'other', 'terms', 'of', 'the', 'issue', '.']\n",
      "['the', 'provision', ',', 'called', 'the', '``', 'two-time-losers', \"''\", 'amendment', 'by', 'its', 'supporters', ',', 'apparently', 'was', 'aimed', 'at', 'preventing', 'texas', 'air', 'corp.', 'chairman', 'frank', 'lorenzo', 'from', 'attempting', 'to', 'take', 'over', 'another', 'airline', '.']\n",
      "['mr.', 'gartner', 'is', 'editor', 'and', 'co-owner', 'of', 'the', 'daily', 'tribune', 'in', 'ames', ',', 'iowa', ',', 'and', 'president', 'of', 'nbc', 'news', 'in', 'new', 'york', '.']\n",
      "['``', 'we', 'play', 'to', 'win', '.']\n",
      "['annualized', 'average', 'rate', 'of', 'return', 'after', 'expenses', 'for', 'the', 'past', '30', 'days', ';', 'not', 'a', 'forecast', 'of', 'future', 'returns', '.']\n",
      "['mr.', 'bernstein', ',', 'a', 'tall', ',', 'energetic', 'man', 'who', 'is', 'widely', 'respected', 'as', 'a', 'publishing', 'executive', ',', 'has', 'spent', 'much', 'of', 'his', 'time', 'in', 'recent', 'years', 'on', 'human', 'rights', 'issues', '.']\n",
      "['his', 'longer', 'analysis', 'of', 'executive', 'power', 'and', 'the', 'appropriations', 'clause', 'is', 'to', 'appear', 'in', 'the', 'duke', 'law', 'journal', 'later', 'this', 'year', '.']\n",
      "['payments', 'are', 'expected', 'to', 'range', 'between', '$', '9', 'billion', 'and', '$', '12', 'billion', 'this', 'year', '.']\n",
      "['``', 'it', 'does', \"n't\", 'make', 'any', 'difference', 'now', '.']\n",
      "['in', 'september', ',', 'the', 'company', 'said', 'it', 'was', 'seeking', 'offers', 'for', 'its', 'five', 'radio', 'stations', 'in', 'order', 'to', 'concentrate', 'on', 'its', 'programming', 'business', '.']\n",
      "['the', 'promotion', 'helped', 'riviera', 'sales', 'exceed', 'the', 'division', \"'s\", 'forecast', 'by', 'more', 'than', '10', '%', ',', 'buick', 'said', 'at', 'the', 'time', '.']\n",
      "['the', 'banks', 'have', '28', 'days', 'to', 'file', 'an', 'appeal', 'against', 'the', 'ruling', 'and', 'are', 'expected', 'to', 'do', 'so', 'shortly', '.']\n",
      "['mr.', 'allen', \"'s\", 'pittsburgh', 'firm', ',', 'advanced', 'investment', 'management', 'inc.', ',', 'executes', 'program', 'trades', 'for', 'institutions', '.']\n",
      "['though', 'some', 'lawyers', 'reported', 'that', 'prospective', 'acquirers', 'were', 'scrambling', 'to', 'make', 'filings', 'before', 'the', 'fees', 'take', 'effect', ',', 'government', 'officials', 'said', 'they', 'had', \"n't\", 'noticed', 'any', 'surge', 'in', 'filings', '.']\n",
      "['freudtoy', ',', 'a', 'pillow', 'bearing', 'the', 'likeness', 'of', 'sigmund', 'freud', ',', 'is', 'marketed', 'as', 'a', '$', '24.95', 'tool', 'for', 'do-it-yourself', 'analysis', '.']\n",
      "['you', 'do', \"n't\", 'want', 'to', 'get', 'yourself', 'too', 'upset', 'about', 'these', 'things', '.']\n",
      "['the', 'underwood', 'family', 'said', 'that', 'holders', 'of', 'more', 'than', 'a', 'majority', 'of', 'the', 'stock', 'of', 'the', 'company', 'have', 'approved', 'the', 'transaction', 'by', 'written', 'consent', '.']\n",
      "['the', 'combined', 'processes', 'may', 'significantly', 'raise', 'the', 'current-carrying', 'capacity', 'of', 'multi-crystal', 'samples', '.']\n",
      "['in', 'american', 'stock', 'exchange', 'composite', 'trading', ',', 'citadel', 'shares', 'closed', 'yesterday', 'at', '$', '45.75', ',', 'down', '25', 'cents', '.']\n",
      "['fees', '1', '3-4', '.']\n",
      "['today', 'taxpayers', 'get', 'to', 'vote', ',', 'most', 'of', 'the', 'time', ',', 'on', 'whether', 'they', 'want', 'to', 'finance', 'the', 'building', 'schemes', 'of', 'our', 'modern', 'political', 'pharaohs', ',', 'or', 'let', 'private', 'money', 'erect', 'these', 'playgrounds', 'for', 'public', 'passions', '.']\n",
      "['earnings', 'were', 'hurt', 'by', 'disposal', 'of', 'operations', 'in', 'its', 'restructuring', ',', 'reed', 'said', '.']\n",
      "['moreover', ',', 'we', \"'ve\", 'probably', 'been', 'the', 'most', 'aggressive', 'firm', 'on', 'the', 'street', 'in', 'reducing', 'costs', ',', 'which', 'are', 'down', 'around', '40', '%', 'over', 'the', 'last', 'six', 'months', '.']\n",
      "['mr.', 'veraldi', 'worked', 'at', 'ford', 'for', '40', 'years', ',', 'holding', 'a', 'variety', 'of', 'car', 'and', 'parts-engineering', 'positions', '.']\n",
      "['they', 'must', 'figure', 'that', 'justice', 'has', 'to', 'get', 'done', 'by', 'somebody', ',', 'but', 'know', 'it', 'wo', \"n't\", 'be', 'done', 'by', 'congress', '.']\n",
      "['may', 'stores', ',', 'st.', 'louis', ',', 'runs', 'such', 'well-known', 'department', 'stores', 'as', 'lord', '&', 'taylor', '.']\n",
      "['sales', ',', 'however', ',', 'were', 'little', 'changed', 'at', '2.46', 'billion', 'guilders', ',', 'compared', 'with', '2.42', 'billion', 'guilders', '.']\n",
      "['cataracts', 'refer', 'to', 'a', 'clouding', 'of', 'the', 'eye', \"'s\", 'natural', 'lens', '.']\n",
      "['it', 'should', 'be', 'constantly', 'stressed', 'that', 'poland', \"'s\", 'farmers', 'mostly', 'need', 'a', 'real', 'market', 'for', 'their', 'products', '.']\n",
      "['the', 'company', \"'s\", 'stock', 'fell', '$', '1.125', 'to', '$', '13.625', 'in', 'over-the-counter', 'trading', 'yesterday', '.']\n",
      "['in', 'anticipation', 'of', 'the', 'start-up', 'of', 'its', 'new', 'factory', ',', 'the', 'company', 'said', 'a', 'larger-than-normal', 'chassis', 'supply', 'has', 'been', 'built', 'to', 'carry', 'it', 'through', 'the', 'transition', 'period', '.']\n",
      "['the', 'percentage', 'change', 'is', 'since', 'year-end', '.']\n",
      "['western', 'gas', 'resources', 'inc.', ',', 'initial', 'offering', 'of', '3,250,000', 'shares', 'of', 'common', 'stock', ',', 'of', 'which', '3,040,000', 'shares', 'will', 'be', 'sold', 'by', 'the', 'company', 'and', '210,000', 'shares', 'by', 'a', 'holder', ',', 'via', 'prudential-bache', 'capital', 'funding', ',', 'smith', 'barney', ',', 'harris', 'upham', '&', 'co.', ',', 'and', 'hanifen', ',', 'imhoff', 'inc', '.']\n",
      "['--', 'pat', \"d'amico\", '.']\n",
      "['arraignments', 'are', 'scheduled', 'for', 'nov.', '14', '.']\n",
      "['in', 'october', ',', '30.6', '%', 'said', 'they', 'will', 'buy', 'appliances', 'in', 'the', 'coming', 'six', 'months', ',', 'compared', 'with', '27.4', '%', 'in', 'september', 'and', '26.5', '%', 'in', 'october', '1988', '.']\n",
      "['the', '12', '%', 'notes', 'due', '1995', 'fell', '9-32', 'to', '103', '3-8', 'to', 'yield', '11.10', '%', '.']\n",
      "['the', 'rating', 'concern', 'said', 'the', 'textile', 'and', 'clothing', 'company', \"'s\", 'interest', 'expense', 'exceeds', 'operating', 'profit', '``', 'by', 'a', 'wide', 'margin', \"''\", 'and', 'it', 'noted', 'united', \"'s\", 'estimated', 'after-tax', 'loss', 'of', '$', '24', 'million', 'for', 'the', 'year', 'ended', 'june', '30', '.']\n",
      "['the', 'year-ago', 'results', 'included', 'a', '$', '415', 'million', 'charge', 'in', 'the', '1988', 'second', 'quarter', 'for', 'underperforming', 'real', 'estate', 'and', 'mortgage', 'loans', '.']\n",
      "['national', 'tyre', ',', 'which', 'has', '420', 'branches', 'throughout', 'the', 'u.k.', ',', 'had', '1988', 'pretax', 'profit', 'of', '#', '8.5', 'million', '.']\n",
      "['lawmakers', 'representing', 'some', 'of', 'the', 'cleaner', 'utilities', 'have', 'been', 'quietly', 'working', 'with', 'the', 'white', 'house', 'to', 'devise', 'ways', 'to', 'tinker', 'with', 'the', 'administration', 'bill', 'to', 'address', 'their', 'acid-rain', 'concerns', '.']\n",
      "['ray', 'shaw', ',', 'chairman', 'of', 'american', 'city', ',', 'said', 'he', 'would', 'assume', 'mr.', 'russell', \"'s\", 'responsibilities', 'if', 'a', 'successor', 'is', \"n't\", 'found', 'this', 'month', '.']\n",
      "['uptick', '--', 'an', 'expression', 'signifying', 'that', 'a', 'transaction', 'in', 'a', 'listed', 'security', 'occurred', 'at', 'a', 'higher', 'price', 'than', 'the', 'previous', 'transaction', 'in', 'that', 'security', '.']\n",
      "['but', 'any', 'potential', 'acquirer', 'must', 'attempt', 'to', 'reach', 'some', 'kind', 'of', 'accord', 'with', 'the', 'company', \"'s\", 'employees', ',', 'primarily', 'its', 'pilots', 'and', 'the', 'powerful', 'machinists', \"'\", 'union', ',', 'which', 'has', 'opposed', 'a', 'takeover', '.']\n",
      "['terms', 'were', \"n't\", 'disclosed', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11672/1367679787.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m                 \u001B[0mcfg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mUNITS\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0munits\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m                 \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnumber_token\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnumber_token\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11672/3705782306.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(verbose, test, number_token, weighted_loss)\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcfg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mEPOCHS\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m         \u001B[0mlog_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_one_epoch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mtest\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_11672/923676452.py\u001B[0m in \u001B[0;36mtrain_one_epoch\u001B[1;34m(model, optimizer, loss_fn, data_loader, device)\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[0mlogprobs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlogprobs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m         \u001B[0mloss_value\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1149\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1150\u001B[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001B[0m\u001B[0;32m   1151\u001B[0m                                \u001B[0mignore_index\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mignore_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreduction\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   2845\u001B[0m         \u001B[0mreduction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlegacy_get_string\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msize_average\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2846\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcross_entropy_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_enum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreduction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_smoothing\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2847\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: Target 41 is out of bounds.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# simple hyperparameter tuning\n",
    "for number_token in (False, True):\n",
    "    for optim in ('rmsprop', 'adam'):\n",
    "        cfg.OPTIM = optim\n",
    "        for hid_size in (64, 128):\n",
    "            cfg.HID_SIZE = hid_size\n",
    "            for (type, rec_size, units) in (('lstm', 1, None), ('lstm', 2, None), ('lstm', 1, 64),\n",
    "                                            ('lstm', 1, 128), ('gru', 1, None)):\n",
    "                cfg.TYPE = type\n",
    "                cfg.REC_SIZE = rec_size\n",
    "                cfg.UNITS = units\n",
    "                train(number_token=number_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test best model\n",
    "# first best model\n",
    "cfg.REC_SIZE = 1\n",
    "cfg.UNITS = 128\n",
    "train(test=True)\n",
    "# second best model\n",
    "cfg.REC_SIZE = 2\n",
    "cfg.UNITS = None\n",
    "train(test=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d92e98c3664063ec1b567951c01aa42f8ffade76e6df5a130cb26ea124003d56"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}