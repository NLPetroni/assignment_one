{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install wandb # colab only\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict, OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "import config as cfg\n",
    "\n",
    "def download_and_unzip(url, save_dir='.'):\n",
    "  # downloads and unzips url, if not already downloaded\n",
    "  # used for downloading dataset and glove embeddings\n",
    "  import os\n",
    "  from urllib.request import urlopen\n",
    "  from io import BytesIO\n",
    "  from zipfile import ZipFile\n",
    "  fname = url.split('/')[-1][:-4] if save_dir == '.' else save_dir\n",
    "  if fname not in os.listdir():\n",
    "    print(f'downloading and unzipping {fname}...', end=' ')\n",
    "    r = urlopen(url)\n",
    "    zipf = ZipFile(BytesIO(r.read()))\n",
    "    zipf.extractall(path=save_dir)\n",
    "    print(f'completed')\n",
    "  else:\n",
    "    print(f'{fname} already downloaded')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def get_wandbkey():\n",
    "    with open('wandbkey.txt') as f:\n",
    "        return f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove(emb_size=100, number_token=False):\n",
    "  \"\"\"\n",
    "    Download and load glove embeddings. \n",
    "    Parameters:\n",
    "      emb_size: embedding size (50/100/200/300-dimensional vectors).    \n",
    "    Returns tuple (voc, emb) where voc is dict from words to idx (in emb) and emb is (numpy) embedding matrix\n",
    "  \"\"\"\n",
    "  n_tokens = 400000 + 1 # glove vocabulary size + PAD\n",
    "  if emb_size not in (50, 100, 200, 300):\n",
    "    raise ValueError(f'wrong size parameter: {emb_size}')\n",
    "  \n",
    "  if number_token: \n",
    "    n_tokens += 1\n",
    "  download_and_unzip('http://nlp.stanford.edu/data/glove.6B.zip', save_dir='glove')\n",
    "  vocabulary = dict()\n",
    "  embedding_matrix = np.ones((n_tokens, emb_size))\n",
    "\n",
    "  with open(f'glove/glove.6B.{emb_size}d.txt', encoding=\"utf8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embedding_matrix[i] = coefs\n",
    "        vocabulary[word] = i\n",
    "  \n",
    "  # add embedding for and padding and number token\n",
    "  if number_token:\n",
    "    embedding_matrix[n_tokens - 2] = 0\n",
    "    vocabulary['<PAD>'] = n_tokens - 2\n",
    "    digits = list(filter(lambda s: re.fullmatch('\\d+(\\.\\d*)?', s) is not None, vocabulary.keys()))\n",
    "    embedding_matrix[n_tokens - 1] = np.mean(embedding_matrix[[vocabulary[d] for d in digits]], axis=0)\n",
    "    vocabulary['<NUM>'] = n_tokens - 1\n",
    "  else: \n",
    "    embedding_matrix[n_tokens - 1] = 0\n",
    "    vocabulary['<PAD>'] = n_tokens - 1\n",
    "  return vocabulary, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Simple dataset class to use dataloaders (batching) \"\"\"\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', \n",
    "        'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', \n",
    "        'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '<PAD>'] # serve una classe per il padding vero?\n",
    "class2idx = {c: i for i, c in enumerate(classes)}\n",
    "\n",
    "\n",
    "def add_oov(start_voc, oovs, embedding_matrix, sentences):\n",
    "  \"\"\"\n",
    "    Computes new embedding matrix, adding embeddings for oovs\n",
    "    Parameters:\n",
    "      start_voc: dict, starting vocabulary that is extended with oovs\n",
    "      oovs: set of string, oovs to add to the starting vocabulary and embedding matrix\n",
    "      embbedding_matrix: starting embedding matrix (numpy)\n",
    "      sentences: list of list of strings, set used to compute oov embeddings\n",
    "    Returns tuple (voc, emb) where voc is dict from words to idx (in emb) and emb is (numpy) embedding matrix with oovs\n",
    "  \"\"\"\n",
    "  oovs = oovs - set(start_voc.keys())\n",
    "  start_voc_size, emb_size = embedding_matrix.shape\n",
    "  oov_embeddings = np.zeros((start_voc_size + len(oovs), emb_size))\n",
    "  oov_embeddings[:start_voc_size] = embedding_matrix\n",
    "  new_voc = dict(start_voc)\n",
    "\n",
    "  for i, oov in enumerate(oovs):\n",
    "    context_words = [new_voc[word] \n",
    "                    for sentence in filter(lambda s: oov in s, sentences)\n",
    "                    for word in sentence if word in new_voc and word not in (oov, '<PAD>')]\n",
    "    oov_embeddings[start_voc_size + i] = np.mean(oov_embeddings[context_words], axis=0)\n",
    "    new_voc[oov] = start_voc_size + i\n",
    "  return new_voc, oov_embeddings\n",
    "    \n",
    "def load_data(start, end, start_voc, embedding_matrix, number_token=False,\n",
    "              drop_punctuation=True, split_docs=True, ret_counts=False):\n",
    "  \"\"\"\n",
    "    Downloads dataset and preprocess data.\n",
    "    Params:\n",
    "      start: idx of first file to include in data\n",
    "      end: idx of last file to include in data\n",
    "      start_voc: starting vocabulary that is extended with oov terms\n",
    "      embedding_matrix: embedding matrix that \n",
    "      #TODO implement number_token: if True, use a single token for all cardinal numbers\n",
    "      drop_punctuation: if True, drop punt\n",
    "      split_docs: if True, each sequence is one sentence; if false, each sequence is one document\n",
    "      ret_counts: if True, also return counts of each word in the documents\n",
    "    Returns \n",
    "  \"\"\"\n",
    "  # download dataset\n",
    "  download_and_unzip('https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip')\n",
    "  \n",
    "  inputs, labels = [], []\n",
    "  vocabulary = set()\n",
    "  counts = defaultdict(int)\n",
    "  \n",
    "  # build dataset\n",
    "  for doc in range(start, end+1):\n",
    "    with open(f'dependency_treebank/wsj_{doc:04d}.dp') as f:\n",
    "      \n",
    "      input_seq, label_seq = [], []\n",
    "      \n",
    "      for line in f:\n",
    "        if line.strip(): # check for empty lines\n",
    "          word, label, _ = line.split('\\t')\n",
    "          word = word.lower()\n",
    "          if '\\/' in word:\n",
    "            word = word.replace('\\/', '-')\n",
    "          if number_token and re.fullmatch('\\d+(\\.\\d*)?', word) is not None:\n",
    "            word = '<NUM>'\n",
    "          if not drop_punctuation or label.isalpha(): # eventually drop punctuation\n",
    "            vocabulary.add(word)\n",
    "            input_seq.append(word)\n",
    "            label_seq.append(label)\n",
    "            counts[word] += 1\n",
    "        elif split_docs: # sentence over, add to input if splitting documents\n",
    "          inputs.append(input_seq)\n",
    "          labels.append(label_seq)\n",
    "          input_seq, label_seq = [], []\n",
    "\n",
    "      inputs.append(input_seq)\n",
    "      labels.append(label_seq)\n",
    "  \n",
    "  max_seq_len = int(np.quantile([len(seq) for seq in inputs], 0.999))\n",
    "  inputs_copy = []\n",
    "  labels_copy = []\n",
    "  for i_seq, l_seq in zip(inputs, labels):\n",
    "    if len(i_seq) > max_seq_len:\n",
    "        inputs_copy.append(i_seq[:max_seq_len])\n",
    "        labels_copy.append(l_seq[:max_seq_len])\n",
    "    else:\n",
    "        inputs_copy.append(i_seq + ['<PAD>'] * (max_seq_len - len(i_seq)))\n",
    "        labels_copy.append(l_seq + ['<PAD>'] * (max_seq_len - len(l_seq)))\n",
    "  \n",
    "  inputs = inputs_copy\n",
    "  labels = labels_copy\n",
    "  vocabulary, embedding_matrix = add_oov(start_voc, vocabulary, embedding_matrix, inputs)\n",
    "  inputs = torch.as_tensor([[vocabulary[word] for word in sequence] for sequence in inputs])\n",
    "  labels = torch.as_tensor([[class2idx[label] for label in sequence] for sequence in labels])\n",
    "\n",
    "  if ret_counts:\n",
    "    return inputs, labels, vocabulary, embedding_matrix, counts\n",
    "  else:\n",
    "    return inputs, labels, vocabulary, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, embedding_matrix, type, rec_size=1, units=None, hid_size=50):\n",
    "    \"\"\"\n",
    "      A recurrent network performing multiclass classification (POS tagging).\n",
    "      Params:\n",
    "        type: type of rnn, either 'lstm' or 'gru'\n",
    "        embedding_matrix: embedding matrix for embedding layer\n",
    "        rec_size: number of stacked recurrent modules\n",
    "        units: int or None, if given then add one additional linear layer with given number of units\n",
    "        hid_size: size of hidden state of recurrent module\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    emb_size = embedding_matrix.shape[1]\n",
    "    self.emb_layer = nn.Embedding.from_pretrained(torch.as_tensor(embedding_matrix))\n",
    "\n",
    "    if type == 'lstm':\n",
    "      rec_module = nn.LSTM\n",
    "    elif type == 'gru':\n",
    "      rec_module = nn.GRU\n",
    "    else:\n",
    "      raise ValueError(f'wrong type {type}, either lstm or gru')\n",
    "    self.rec_modules = rec_module(input_size=emb_size, hidden_size=hid_size, bidirectional=True, batch_first=True, num_layers=rec_size)\n",
    "\n",
    "    fc_params = [2 * hid_size] + ([units, 37] if units is not None else [37])\n",
    "    self.fc_modules = nn.Sequential(\n",
    "      OrderedDict([(f'fc_{i}', nn.Linear(in_shape, out_shape)) \n",
    "      for i, (in_shape, out_shape) in enumerate(zip(fc_params[:-1], fc_params[1:]))]))\n",
    "      \n",
    "    # self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    vecs = self.emb_layer(x).float()\n",
    "    rec_out, _ = self.rec_modules(vecs)\n",
    "    fc_out = self.fc_modules(rec_out)\n",
    "    return fc_out\n",
    "    # return self.logsoftmax(fc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loss_fn, data_loader, device):\n",
    "    \"\"\" \n",
    "        Trains model for one epoch on the given dataloader.\n",
    "        Parameters:\n",
    "            model: torch.nn.Module to train\n",
    "            optimizer: torch.optim optimizer object\n",
    "            loss_fn: torch.nn criterion to use to compute loss, given outputs and targets\n",
    "            data_loader: torch.utils.data.DataLoader \n",
    "            device: torch.device where training is performed\n",
    "        Returns log dict {'train/loss' : list(loss values for each batch)} \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    log_dict = {'train/loss': []}\n",
    "\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logprobs = model(inputs).transpose(1, 2)\n",
    "        loss = loss_fn(logprobs, targets)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log_dict['train/loss'].append(loss_value)\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "def evaluate(model, loss_fn, data_loader, device, metric='accuracy'):\n",
    "    \"\"\" \n",
    "        Evaluate model on the given dataloader.\n",
    "        Parameters:\n",
    "            model: torch.nn.Module to evaluate\n",
    "            loss_fn: torch.nn criterion to use to compute loss, given outputs and targets\n",
    "            data_loader: torch.utils.data.DataLoader \n",
    "            device: torch.device where evaluation is performed\n",
    "            metric: either 'accuracy' or 'f1'\n",
    "        Returns log dict {'valid/loss' : mean loss, 'valid/{metric}': mean metric} \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    batch_metrics = []\n",
    "    if metric == 'f1':\n",
    "        assert len(data_loader) == 1 # must be a single batch\n",
    "        split = 'test'\n",
    "    else:\n",
    "        split = 'valid'\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logprobs = model(inputs).transpose(1, 2)\n",
    "            loss_value = loss_fn(logprobs, targets).item()\n",
    "            preds = torch.argmax(logprobs, 1)\n",
    "\n",
    "            if metric == 'accuracy':\n",
    "                metric_value = ((targets == preds).sum() / (data_loader.batch_size * targets.shape[1])).item()\n",
    "            elif metric == 'f1':\n",
    "                metric_value = f1_score(targets.cpu().numpy().reshape(-1),\n",
    "                                        preds.cpu().numpy().reshape(-1),\n",
    "                                        labels=list(class2idx.values()),\n",
    "                                        average='macro', zero_division=1)\n",
    "            else:\n",
    "                raise ValueError(f'wrong metric {metric}, either accuracy or f1')\n",
    "\n",
    "            batch_losses.append(loss_value)\n",
    "            batch_metrics.append(metric_value)\n",
    "\n",
    "    log_dict = {f'{split}/loss': np.mean(batch_losses),\n",
    "               f'{split}/{metric}': np.mean(batch_metrics) if metric == 'accuracy' else batch_metrics[0]}\n",
    "    return log_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(verbose=False, test=False, number_token=False):\n",
    "    \"\"\" Fully trains one model, based on cfg parameters, on training set and performs evaluation on validation set.\n",
    "        Returns trained model.\n",
    "    \"\"\"\n",
    "    cfg_dict = {\n",
    "        'epochs': cfg.EPOCHS, 'batch_size': cfg.BATCH_SIZE, 'number_token': number_token,\n",
    "        'model': cfg.TYPE, 'rec_size': cfg.REC_SIZE, 'units': cfg.UNITS, 'hid_size': cfg.HID_SIZE,\n",
    "        'optim': cfg.OPTIM, 'lr': cfg.LR, 'alpha': cfg.ALPHA, 'betas': cfg.BETAS, 'momentum': cfg.MOMENTUM, 'weight_decay': cfg.WEIGHT_DECAY\n",
    "    }\n",
    "    if verbose:\n",
    "        print('CONFIG PARAMETERS:')\n",
    "        pprint(cfg_dict)\n",
    "    metric = 'f1' if test else 'accuracy'\n",
    "    wandb.login(key=get_wandbkey())\n",
    "    run = wandb.init(project=\"assignment-one\", entity=\"nlpetroni\", reinit=True, config=cfg_dict)\n",
    "    wandb.define_metric(\"train_step\")\n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric('train/loss', step_metric=\"train_step\", summary=\"min\")\n",
    "    wandb.define_metric(f\"valid/loss\", step_metric=\"epoch\", summary=\"min\")\n",
    "    wandb.define_metric(f\"valid/accuracy\", step_metric=\"epoch\", summary=\"max\")\n",
    "\n",
    "    glove_voc, embedding_matrix = get_glove(number_token=number_token)\n",
    "    if not test:\n",
    "        train_set, train_labels, train_voc, embedding_matrix = load_data(1, 100, glove_voc, embedding_matrix, number_token=number_token)\n",
    "        valid_set, valid_labels, valid_voc, embedding_matrix = load_data(101, 150, train_voc, embedding_matrix, number_token=number_token)\n",
    "        train_dl = torch.utils.data.DataLoader(Dataset(train_set, train_labels), batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "        valid_dl = torch.utils.data.DataLoader(Dataset(valid_set, valid_labels), batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "    else:\n",
    "        train_set, train_labels, train_voc, embedding_matrix = load_data(1, 150, glove_voc, embedding_matrix, number_token=number_token)\n",
    "        test_set, test_labels, test_voc, embedding_matrix = load_data(151, 200, train_voc, embedding_matrix, number_token=number_token)\n",
    "        train_dl = torch.utils.data.DataLoader(Dataset(train_set, train_labels), batch_size=cfg.BATCH_SIZE, shuffle=True)\n",
    "        test_dl = torch.utils.data.DataLoader(Dataset(test_set, test_labels), batch_size=test_set.shape[0], shuffle=True)\n",
    "\n",
    "\n",
    "    model = POSTagger(embedding_matrix, type=cfg.TYPE, rec_size=cfg.REC_SIZE, units=cfg.UNITS, hid_size=cfg.HID_SIZE).to(device)\n",
    "    wandb.watch(model, log_graph=True)\n",
    "    if verbose:\n",
    "        print(summary(model))\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if cfg.OPTIM == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(params, lr=cfg.LR, alpha=cfg.ALPHA, momentum=cfg.MOMENTUM, weight_decay=cfg.WEIGHT_DECAY)\n",
    "    elif cfg.OPTIM == 'adam':\n",
    "        optimizer = torch.optim.Adam(params, lr=cfg.LR, betas=cfg.BETAS, weight_decay=cfg.WEIGHT_DECAY)\n",
    "    else:\n",
    "        raise ValueError(f'wrong optim {cfg.OPTIM}, either rmsprop or adam')\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_step = 0\n",
    "    print('STARTING TRAINING')\n",
    "    \n",
    "    for epoch in range(cfg.EPOCHS):\n",
    "        log_dict = train_one_epoch(model, optimizer, loss, train_dl, device)\n",
    "        if not test:\n",
    "            log_dict.update(evaluate(model, loss, valid_dl, device, metric=metric))\n",
    "        for batch_loss in log_dict['train/loss']:\n",
    "            wandb.log({'train_step': train_step, 'epoch': epoch, 'train/loss': batch_loss})\n",
    "            train_step += 1\n",
    "        wandb.log({'epoch': epoch, 'valid/loss': log_dict['valid/loss'], 'valid/accuracy': log_dict['valid/accuracy']})\n",
    "        if (epoch % 25) == 0:\n",
    "            print(f'[{epoch:03d}/{cfg.EPOCHS:03d}] train loss: {np.mean(log_dict[\"train/loss\"]):.3f}, valid loss: {log_dict[\"valid/loss\"]:.3f}, accuracy: {log_dict[\"valid/accuracy\"]:.2f}')\n",
    "    if test:\n",
    "        log_dict = evaluate(model, loss, test_dl, device, metric=metric)\n",
    "        wandb.log()\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple hyperparameter tuning\n",
    "for number_token in (False, True):\n",
    "    for lr in (0.001, 0.0005):\n",
    "        cfg.LR = lr\n",
    "        for batch_size in (32, 64, 128):\n",
    "            cfg.BATCH_SIZE = batch_size\n",
    "            for optim in ('rmsprop', 'adam'):\n",
    "                cfg.OPTIM = optim\n",
    "                for hid_size in (64, 128):\n",
    "                    cfg.HID_SIZE = hid_size\n",
    "                    for (type, rec_size, units) in (('lstm', 1, None), ('lstm', 2, None), ('lstm', 1, 64),\n",
    "                                                    ('lstm', 1, 128), ('gru', 1, None)):\n",
    "                        cfg.TYPE = type\n",
    "                        cfg.REC_SIZE = rec_size\n",
    "                        cfg.UNITS = units\n",
    "                        train(number_token=number_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple hyperparameter tuning\n",
    "for number_token in (False, True):\n",
    "    for lr in (0.005, 0.001, 0.0005):\n",
    "        cfg.LR = lr\n",
    "        for batch_size in (32, 64, 128):\n",
    "            cfg.BATCH_SIZE = batch_size\n",
    "            for optim in ('rmsprop', 'adam'):\n",
    "                cfg.OPTIM = optim\n",
    "                for hid_size in (32, 64, 128, 256):\n",
    "                    cfg.HID_SIZE = hid_size\n",
    "                    for (type, rec_size, units) in (('lstm', 1, None), ('lstm', 2, None), ('lstm', 1, 64),\n",
    "                                                    ('lstm', 1, 128), ('gru', 1, None)):\n",
    "                        cfg.TYPE = type\n",
    "                        cfg.REC_SIZE = rec_size\n",
    "                        cfg.UNITS = units\n",
    "                        train(number_token=number_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test best model\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d92e98c3664063ec1b567951c01aa42f8ffade76e6df5a130cb26ea124003d56"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
