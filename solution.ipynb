{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install wandb # colab only\n",
    "import re\n",
    "import os\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score\n",
    "import config as cfg\n",
    "\n",
    "def download_and_unzip(url, save_dir='.'):\n",
    "  # downloads and unzips url, if not already downloaded\n",
    "  # used for downloading dataset and glove embeddings\n",
    "  import os\n",
    "  from urllib.request import urlopen\n",
    "  from io import BytesIO\n",
    "  from zipfile import ZipFile\n",
    "  fname = url.split('/')[-1][:-4] if save_dir == '.' else save_dir\n",
    "  if fname not in os.listdir():\n",
    "    print(f'downloading and unzipping {fname}...', end=' ')\n",
    "    r = urlopen(url)\n",
    "    zipf = ZipFile(BytesIO(r.read()))\n",
    "    zipf.extractall(path=save_dir)\n",
    "    print(f'completed')\n",
    "  else:\n",
    "    print(f'{fname} already downloaded')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def get_wandbkey():\n",
    "    with open('wandbkey.txt') as f:\n",
    "        return f.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 400000 # TODO used by collate_fn, can be done better probably\n",
    "def get_glove(emb_size=100, number_token=False):\n",
    "  \"\"\"\n",
    "    Download and load glove embeddings. \n",
    "    Parameters:\n",
    "      emb_size: embedding size (50/100/200/300-dimensional vectors).    \n",
    "    Returns tuple (voc, emb) where voc is dict from words to idx (in emb) and emb is (numpy) embedding matrix\n",
    "  \"\"\"\n",
    "  n_tokens = 400000 + 1 # glove vocabulary size + PAD\n",
    "  if emb_size not in (50, 100, 200, 300):\n",
    "    raise ValueError(f'wrong size parameter: {emb_size}')\n",
    "  \n",
    "  if number_token: \n",
    "    n_tokens += 1\n",
    "  download_and_unzip('http://nlp.stanford.edu/data/glove.6B.zip', save_dir='glove')\n",
    "  vocabulary = dict()\n",
    "  embedding_matrix = np.ones((n_tokens, emb_size))\n",
    "\n",
    "  with open(f'glove/glove.6B.{emb_size}d.txt', encoding=\"utf8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embedding_matrix[i] = coefs\n",
    "        vocabulary[word] = i\n",
    "  \n",
    "  # add embedding for and padding and number token\n",
    "  if number_token:\n",
    "    embedding_matrix[n_tokens - 2] = 0\n",
    "    vocabulary['<PAD>'] = n_tokens - 2\n",
    "    digits = list(filter(lambda s: re.fullmatch('\\d+(\\.\\d*)?', s) is not None, vocabulary.keys()))\n",
    "    embedding_matrix[n_tokens - 1] = np.mean(embedding_matrix[[vocabulary[d] for d in digits]], axis=0)\n",
    "    vocabulary['<NUM>'] = n_tokens - 1\n",
    "  else: \n",
    "    embedding_matrix[n_tokens - 1] = 0\n",
    "    vocabulary['<PAD>'] = n_tokens - 1\n",
    "  return vocabulary, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'VBG': 0, '.': 1, 'JJS': 2, 'VBD': 3, 'RBR': 4, 'VB': 5, 'NNS': 6, 'JJR': 7, '$': 8, 'WP': 9, 'WDT': 10, 'RBS': 11, '-LRB-': 12, 'FW': 13, 'DT': 14, '#': 15, 'PRP': 16, 'PDT': 17, 'VBZ': 18, 'JJ': 19, 'RB': 20, ',': 21, 'POS': 22, 'UH': 23, 'WRB': 24, 'VBN': 25, 'SYM': 26, 'PRP$': 27, 'VBP': 28, 'TO': 29, 'MD': 30, 'EX': 31, '-RRB-': 32, 'RP': 33, 'NN': 34, '``': 35, 'IN': 36, 'CC': 37, 'NNP': 38, 'NNPS': 39, \"''\": 40, ':': 41, '<PAD>': 42, 'WP$': 43, 'CD': 44, 'LS': 45}\n"
     ]
    }
   ],
   "source": [
    "def load_classes():\n",
    "    download_and_unzip('https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip')\n",
    "    c = set()\n",
    "    for doc in range(1, 200):\n",
    "        with open(f'dependency_treebank/wsj_{doc:04d}.dp') as f:\n",
    "            for line in f:\n",
    "                if line.strip(): # check for empty lines\n",
    "                    _, label, _ = line.split('\\t')\n",
    "                    #print(label)\n",
    "                    c.add(label)\n",
    "    return c\n",
    "\n",
    "classes = {'$', 'NN', ',', 'RBS', 'FW', 'CC', '#', 'VBD', 'PRP', 'RBR', 'LS', ':', 'VBZ', 'MD',\n",
    "           'EX', 'RB', 'WRB', 'NNS', 'VBG', 'PRP$', 'JJR', 'WP$', 'WP', '-LRB-', 'WDT', '``',\n",
    "           '.', 'CD', 'JJ', \"''\", 'UH', 'VBN', 'IN', 'SYM', 'DT', 'JJS', '-RRB-', 'RP', 'VB',\n",
    "           'POS', 'NNP', 'PDT', 'NNPS', 'VBP', 'TO', '<PAD>'}\n",
    "punctuation_cls = {'$', ',', '#', ':', '-LRB-', '``', '.', \"''\", 'SYM', '-RRB-', '<PAD>'}\n",
    "class2idx = {c: i for i, c in enumerate(classes)}\n",
    "print(class2idx)\n",
    "\n",
    "def add_oov(start_voc, oovs, embedding_matrix, sentences, verbose=True):\n",
    "  \"\"\"\n",
    "    Computes new embedding matrix, adding embeddings for oovs\n",
    "    Parameters:\n",
    "      start_voc: dict, starting vocabulary that is extended with oovs\n",
    "      oovs: set of string, oovs to add to the starting vocabulary and embedding matrix\n",
    "      embedding_matrix: starting embedding matrix (numpy)\n",
    "      sentences: list of list of strings, set used to compute oov embeddings\n",
    "    Returns tuple (voc, emb) where voc is dict from words to idx (in emb) and emb is (numpy) embedding matrix with oovs\n",
    "  \"\"\"\n",
    "  oovs = oovs - set(start_voc.keys())\n",
    "  start_voc_size, emb_size = embedding_matrix.shape\n",
    "  oov_embeddings = np.zeros((start_voc_size + len(oovs), emb_size))\n",
    "  oov_embeddings[:start_voc_size] = embedding_matrix\n",
    "  new_voc = dict(start_voc)\n",
    "\n",
    "  for i, oov in enumerate(oovs):\n",
    "    context_words = [new_voc[word] \n",
    "                    for sentence in filter(lambda s: oov in s, sentences)\n",
    "                    for word in sentence if word in new_voc and word not in (oov, '<PAD>')]\n",
    "    if verbose and len(context_words) == 0:\n",
    "        print(f'Empty context for oov: {oov}')\n",
    "        print([sentence for sentence in filter(lambda s: oov in s, sentences)])\n",
    "    oov_embeddings[start_voc_size + i] = np.mean(oov_embeddings[context_words], axis=0)\n",
    "    new_voc[oov] = start_voc_size + i\n",
    "  return new_voc, oov_embeddings\n",
    "    \n",
    "def load_data(start, end, start_voc, embedding_matrix, number_token=False,\n",
    "              drop_punctuation=False, split_docs=True, ret_counts=False):\n",
    "  \"\"\"\n",
    "    Downloads dataset and preprocess data.\n",
    "    Params:\n",
    "      start: idx of first file to include in data\n",
    "      end: idx of last file to include in data\n",
    "      start_voc: starting vocabulary that is extended with oov terms\n",
    "      embedding_matrix: embedding matrix that \n",
    "      number_token: if True, use a single token for all cardinal numbers\n",
    "      drop_punctuation: if True, drop punt\n",
    "      split_docs: if True, each sequence is one sentence; if false, each sequence is one document\n",
    "      ret_counts: if True, also return counts of each word in the documents\n",
    "    Returns \n",
    "  \"\"\"\n",
    "  # download dataset\n",
    "  download_and_unzip('https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip')\n",
    "\n",
    "  inputs, labels = [], []\n",
    "  vocabulary = set()\n",
    "  counts = defaultdict(int)\n",
    "  \n",
    "  # build dataset\n",
    "  for doc in range(start, end+1):\n",
    "    with open(f'dependency_treebank/wsj_{doc:04d}.dp') as f:\n",
    "      \n",
    "      input_seq, label_seq = [], []\n",
    "      \n",
    "      for line in f:\n",
    "        if line.strip(): # check for empty lines\n",
    "          word, label, _ = line.split('\\t')\n",
    "          word = word.lower()\n",
    "          if '\\/' in word:\n",
    "            word = word.replace('\\/', '-')\n",
    "          if number_token and re.fullmatch('\\d+(\\.\\d*)?', word) is not None:\n",
    "            word = '<NUM>'\n",
    "          if not drop_punctuation or label.isalpha(): # eventually drop punctuation\n",
    "            vocabulary.add(word)\n",
    "            input_seq.append(word)\n",
    "            label_seq.append(label)\n",
    "            counts[word] += 1\n",
    "        elif split_docs: # sentence over, add to input if splitting documents\n",
    "          inputs.append(input_seq)\n",
    "          labels.append(label_seq)\n",
    "          input_seq, label_seq = [], []\n",
    "\n",
    "      inputs.append(input_seq)\n",
    "      labels.append(label_seq)\n",
    "\n",
    "  vocabulary, embedding_matrix = add_oov(start_voc, vocabulary, embedding_matrix, inputs)\n",
    "\n",
    "  if ret_counts:\n",
    "    return inputs, labels, vocabulary, embedding_matrix, counts\n",
    "  else:\n",
    "    return inputs, labels, vocabulary, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class POSDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Simple dataset class to use dataloaders (batching) \"\"\"\n",
    "    def __init__(self, inputs, labels, vocabulary):\n",
    "        self.inputs_str = inputs\n",
    "        self.labels_str = labels\n",
    "        self.voc = vocabulary\n",
    "        self.inputs = [[vocabulary[word] for word in sequence] for sequence in inputs]\n",
    "        self.labels = [[class2idx[label] for label in sequence] for sequence in labels]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Used by DataLoader to pad batches\"\"\"\n",
    "    max_seq_len = int(np.quantile([len(sample[0]) for sample in batch], 0.99))\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i_seq, t_seq in batch:\n",
    "        if len(i_seq) > max_seq_len:\n",
    "            inputs.append(i_seq[:max_seq_len])\n",
    "            targets.append(t_seq[:max_seq_len])\n",
    "        else:\n",
    "            inputs.append(i_seq + [PAD_TOKEN] * (max_seq_len - len(i_seq))) # sì brutto con train_voc da fuori ma non mi viene meglio ora\n",
    "            targets.append(t_seq + [class2idx['<PAD>']] * (max_seq_len - len(t_seq)))\n",
    "    return torch.as_tensor(inputs), torch.as_tensor(targets)\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, model, delta=0, path='res'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.model = model\n",
    "        self.path = path\n",
    "        if not os.path.isdir(self.path):\n",
    "            os.mkdir(self.path)\n",
    "        self.best_score = float('inf')\n",
    "        self.counter = 0\n",
    "    \n",
    "    def step(self, epoch, score):\n",
    "        \"\"\" Updates ES tracker after one epoch.\n",
    "            Params:\n",
    "                epoch: current epoch\n",
    "                score: validation loss\n",
    "            Returns tuple (stop, checkpoint), \n",
    "                where stop is True if early stopping has occurred and False otherwise,\n",
    "                and checkpoint is last best checkpoint\n",
    "        \"\"\"\n",
    "        if score < self.best_score:\n",
    "            # print('Validation loss decreasing, storing new checkpoint')\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            checkpoint = {'model': self.model.state_dict(), 'epoch': epoch}\n",
    "            torch.save(checkpoint, 'res/checkpoint.pth')\n",
    "            return False, checkpoint\n",
    "        elif abs(score - self.best_score) > self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f'Early stopping occured at epoch {epoch} with patience {self.patience}')\n",
    "                checkpoint = torch.load('res/checkpoint.pth', map_location='cpu')\n",
    "                return True, checkpoint\n",
    "            print(f'Validation loss increasing for {self.counter} epochs')\n",
    "            return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTagger(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, embedding_matrix, type, rec_size=1, units=None, hid_size=50):\n",
    "    \"\"\"\n",
    "      A recurrent network performing multiclass classification (POS tagging).\n",
    "      Params:\n",
    "        type: type of rnn, either 'lstm' or 'gru'\n",
    "        embedding_matrix: embedding matrix for embedding layer\n",
    "        rec_size: number of stacked recurrent modules\n",
    "        units: int or None, if given then add one additional linear layer with given number of units\n",
    "        hid_size: size of hidden state of recurrent module\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    emb_size = embedding_matrix.shape[1]\n",
    "    self.emb_layer = nn.Embedding.from_pretrained(torch.as_tensor(embedding_matrix))\n",
    "\n",
    "    if type == 'lstm':\n",
    "      rec_module = nn.LSTM\n",
    "    elif type == 'gru':\n",
    "      rec_module = nn.GRU\n",
    "    else:\n",
    "      raise ValueError(f'wrong type {type}, either lstm or gru')\n",
    "    self.rec_modules = rec_module(input_size=emb_size, hidden_size=hid_size, bidirectional=True, batch_first=True, num_layers=rec_size)\n",
    "    cls = len(classes)\n",
    "    self.fc_modules = nn.Sequential(nn.Linear(2 * hid_size, units if units is not None else cls))\n",
    "    if units is not None:\n",
    "      self.fc_modules.add_module(nn.ReLU())\n",
    "      self.fc_modules.add_module(nn.Linear(units, cls))\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    vecs = self.emb_layer(x).float()\n",
    "    rec_out, _ = self.rec_modules(vecs)\n",
    "    fc_out = self.fc_modules(rec_out)\n",
    "    return fc_out\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, ignore_index=-100, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, inputs, targets):    \n",
    "        ce_loss = nn.CrossEntropyLoss(ignore_index=self.ignore_index, reduction='none')(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, loss_fn, data_loader, device):\n",
    "    \"\"\" \n",
    "        Trains model for one epoch on the given dataloader.\n",
    "        Parameters:\n",
    "            model: torch.nn.Module to train\n",
    "            optimizer: torch.optim optimizer object\n",
    "            loss_fn: torch.nn criterion to use to compute loss, given outputs and targets\n",
    "            data_loader: torch.utils.data.DataLoader \n",
    "            device: torch.device where training is performed\n",
    "        Returns log dict {'train/loss' : list(loss values for each batch)} \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    log_dict = {'train/loss': []}\n",
    "\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logprobs = model(inputs).transpose(1, 2)\n",
    "        loss = loss_fn(logprobs, targets)\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            exit(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        log_dict['train/loss'].append(loss_value)\n",
    "\n",
    "    return log_dict\n",
    "\n",
    "def evaluate(model, loss_fn, data_loader, device, split, ret_f1_classes=False):\n",
    "    \"\"\" \n",
    "        Evaluate model on the given dataloader.\n",
    "        Parameters:\n",
    "            model: torch.nn.Module to evaluate\n",
    "            loss_fn: torch.nn criterion to use to compute loss, given outputs and targets\n",
    "            data_loader: torch.utils.data.DataLoader \n",
    "            device: torch.device where evaluation is performed\n",
    "            split: either 'valid' or 'test'\n",
    "            ret_f1_classes: if True, also returns per-class f1 scores\n",
    "        Returns log dict {'valid/loss' : mean loss, 'valid/{metric}': mean metric} \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    assert len(data_loader) == 1 # must be a single batch\n",
    "    with torch.no_grad():\n",
    "        inputs, targets = next(iter(data_loader))\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        scores = model(inputs).transpose(1, 2)\n",
    "        losses = loss_fn(scores, targets).item()\n",
    "        preds = torch.argmax(scores, 1)\n",
    "\n",
    "        targets = targets.cpu().numpy()\n",
    "        preds = preds.cpu().numpy()\n",
    "        mask = [targets != class2idx[c] for c in punctuation_cls]\n",
    "        mask = np.array(reduce(lambda a,b: a & b, mask)).reshape(targets.shape)\n",
    "        acc = np.where(mask, targets==preds, False).sum() / mask.sum()\n",
    "        cls = [class2idx[c] for c in (classes - punctuation_cls)]\n",
    "        f1_classes = f1_score(targets.reshape(-1), preds.reshape(-1),\n",
    "                      labels=cls, average=None, zero_division=1)\n",
    "\n",
    "    log_dict = {f'{split}/loss': losses,\n",
    "                f'{split}/accuracy': acc,\n",
    "                f'{split}/f1': np.mean(f1_classes)}\n",
    "    if ret_f1_classes:\n",
    "        return log_dict, {c:s for c,s in zip(cls, f1_classes)}\n",
    "    else:\n",
    "        return log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(tags=None, verbose=False, test=False, number_token=False, focal_loss=False):\n",
    "    \"\"\" Fully trains one model, based on cfg parameters, on training set and performs evaluation on validation set.\n",
    "        Returns trained model.\n",
    "    \"\"\"\n",
    "    idx2classes = {i: c for c, i in class2idx.items()}\n",
    "    cfg_dict = {\n",
    "        'epochs': cfg.EPOCHS, 'batch_size': cfg.BATCH_SIZE, 'number_token': number_token,\n",
    "        'model': cfg.TYPE, 'rec_size': cfg.REC_SIZE, 'units': cfg.UNITS, 'hid_size': cfg.HID_SIZE,\n",
    "        'optim': cfg.OPTIM, 'lr': cfg.LR, 'alpha': cfg.ALPHA, 'betas': cfg.BETAS, 'momentum': cfg.MOMENTUM, 'focal_loss': focal_loss\n",
    "    }\n",
    "    if verbose:\n",
    "        print('CONFIG PARAMETERS:')\n",
    "        pprint(cfg_dict)\n",
    "    wandb.login(key=get_wandbkey())\n",
    "    run = wandb.init(project=\"assignment-one\", entity=\"nlpetroni\", group=f'{\"testing\" if test else \"validation\"}',\n",
    "                     reinit=True, config=cfg_dict, tags=tags)\n",
    "    wandb.define_metric(\"train_step\")\n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric('train/loss', step_metric=\"train_step\", summary=\"min\")\n",
    "    wandb.define_metric(f\"valid/loss\", step_metric=\"epoch\", summary=\"min\")\n",
    "    wandb.define_metric(f\"valid/accuracy\", step_metric=\"epoch\", summary=\"max\")\n",
    "    wandb.define_metric(f\"valid/f1\", step_metric=\"epoch\", summary=\"max\")\n",
    "    wandb.define_metric(f\"test/accuracy\", step_metric=\"epoch\", summary=\"max\")\n",
    "    wandb.define_metric(f\"test/f1\", step_metric=\"epoch\", summary=\"max\")\n",
    "\n",
    "    glove_voc, embedding_matrix = get_glove(number_token=number_token)\n",
    "    if not test:\n",
    "        split = 'valid'\n",
    "        train_set, train_labels, train_voc, embedding_matrix = load_data(1, 100, glove_voc, embedding_matrix, number_token=number_token, drop_punctuation=False)\n",
    "        valid_set, valid_labels, valid_voc, embedding_matrix = load_data(101, 150, train_voc, embedding_matrix, number_token=number_token, drop_punctuation=False)\n",
    "        train_ds = POSDataset(train_set, train_labels, train_voc)\n",
    "        valid_ds = POSDataset(valid_set, valid_labels, valid_voc)\n",
    "        train_dl = torch.utils.data.DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "        valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=len(valid_ds), collate_fn=collate_fn)\n",
    "    else:\n",
    "        split = 'test'\n",
    "        train_set, train_labels, train_voc, embedding_matrix = load_data(1, 150, glove_voc, embedding_matrix, number_token=number_token, drop_punctuation=False)\n",
    "        test_set, test_labels, test_voc, embedding_matrix = load_data(151, 199, train_voc, embedding_matrix, number_token=number_token, drop_punctuation=False)\n",
    "        train_ds = POSDataset(train_set, train_labels, train_voc)\n",
    "        test_ds = POSDataset(test_set, test_labels, test_voc)\n",
    "        train_dl = torch.utils.data.DataLoader(train_ds, batch_size=cfg.BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "        test_dl = torch.utils.data.DataLoader(test_ds, batch_size=len(test_ds), collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "    model = POSTagger(embedding_matrix, type=cfg.TYPE, rec_size=cfg.REC_SIZE, units=cfg.UNITS, hid_size=cfg.HID_SIZE).to(device)\n",
    "    wandb.watch(model, log_graph=True)\n",
    "    if verbose:\n",
    "        print(summary(model))\n",
    "\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    if cfg.OPTIM == 'rmsprop':\n",
    "        optimizer = torch.optim.RMSprop(params, lr=cfg.LR, alpha=cfg.ALPHA, momentum=cfg.MOMENTUM, weight_decay=cfg.WEIGHT_DECAY)\n",
    "    elif cfg.OPTIM == 'adam':\n",
    "        optimizer = torch.optim.Adam(params, lr=cfg.LR, betas=cfg.BETAS, weight_decay=cfg.WEIGHT_DECAY)\n",
    "    else:\n",
    "        raise ValueError(f'wrong optim {cfg.OPTIM}, either rmsprop or adam')\n",
    "    if focal_loss: \n",
    "        loss_fn = FocalLoss(ignore_index=class2idx['<PAD>'])\n",
    "    else:\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=class2idx['<PAD>']) # ignore padding\n",
    "    train_step = 0\n",
    "    es_tracker = EarlyStopping(10, model)\n",
    "    epoch = 0\n",
    "    stop = False\n",
    "    print('STARTING TRAINING')\n",
    "    while epoch < cfg.EPOCHS and not stop:\n",
    "        train_log_dict = train_one_epoch(model, optimizer, loss_fn, train_dl, device)\n",
    "        if not test:\n",
    "            valid_log_dict, f1_classes = evaluate(model, loss_fn, valid_dl, device, split=split, ret_f1_classes=True)\n",
    "            stop, checkpoint = es_tracker.step(epoch, valid_log_dict['valid/loss'])\n",
    "            wandb.log({'epoch': epoch, 'valid/loss': valid_log_dict['valid/loss'],\n",
    "                       'valid/accuracy': valid_log_dict['valid/accuracy'], 'valid/f1': valid_log_dict['valid/f1'],\n",
    "                       'valid/f1_distribution': wandb.Histogram(np_histogram=np.histogram(list(f1_classes.values())))})\n",
    "            if stop:\n",
    "                model.load_state_dict(checkpoint['model'])\n",
    "            if (epoch % 25) == 0:\n",
    "                print(f'[{epoch:03d}/{cfg.EPOCHS:03d}] train loss: {np.mean(train_log_dict[\"train/loss\"]):.3f}, valid loss: {valid_log_dict[\"valid/loss\"]:.3f}, f1: {valid_log_dict[\"valid/f1\"]:.2f} accuracy: {valid_log_dict[\"valid/accuracy\"]:.2f}')\n",
    "        for batch_loss in train_log_dict['train/loss']:\n",
    "            wandb.log({'train_step': train_step, 'epoch': epoch, 'train/loss': batch_loss})\n",
    "            train_step += 1\n",
    "        epoch += 1\n",
    "    # log per-class f1 scores\n",
    "    data = [[idx2classes[i], score] for i, score in f1_classes.items()]\n",
    "    table = wandb.Table(data=data, columns=[\"class\", \"f1_score\"])\n",
    "    wandb.log({'valid/f1_per_class': wandb.plot.bar(table, \"class\", \"f1_score\", title=\"F1 per class bar chart\")})\n",
    "    if test:\n",
    "        log_dict, f1_classes = evaluate(model, loss_fn, test_dl, device, split=split, ret_f1_classes=True)\n",
    "        data = [[idx2classes[i], score] for i,score in f1_classes.items()]\n",
    "        table = wandb.Table(data=data, columns = [\"class\", \"f1_score\"])\n",
    "        wandb.log({'test/loss': log_dict['test/loss'], 'test/accuracy': log_dict['test/accuracy'], 'test/f1': log_dict['test/f1'],\n",
    "                   'test/f1_per_class': wandb.plot.bar(table, \"class\", \"f1_score\", title=\"F1 per class bar chart\")})\n",
    "\n",
    "    run.finish()\n",
    "\n",
    "    return model, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiegochine\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/2npa23wl\" target=\"_blank\">sleek-cosmos-1016</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 1.636, valid loss: 1.308, f1: 0.01 accuracy: 0.17\n",
      "[025/200] train loss: 0.212, valid loss: 0.196, f1: 0.47 accuracy: 0.81\n",
      "[050/200] train loss: 0.106, valid loss: 0.110, f1: 0.61 accuracy: 0.86\n",
      "[075/200] train loss: 0.073, valid loss: 0.087, f1: 0.67 accuracy: 0.88\n",
      "[100/200] train loss: 0.055, valid loss: 0.076, f1: 0.71 accuracy: 0.89\n",
      "[125/200] train loss: 0.044, valid loss: 0.071, f1: 0.74 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[150/200] train loss: 0.035, valid loss: 0.068, f1: 0.74 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[175/200] train loss: 0.028, valid loss: 0.066, f1: 0.74 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 198 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9127... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697bb46ba7d84dedbd2288acd86ba35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▄▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▅▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>valid/loss</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>198</td></tr><tr><td>train_step</td><td>12337</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sleek-cosmos-1016</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/2npa23wl\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/2npa23wl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_002556-2npa23wl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/5orh5r6w\" target=\"_blank\">stoic-planet-1017</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 1.484, valid loss: 1.124, f1: 0.01 accuracy: 0.20\n",
      "[025/200] train loss: 0.126, valid loss: 0.127, f1: 0.57 accuracy: 0.85\n",
      "[050/200] train loss: 0.067, valid loss: 0.084, f1: 0.66 accuracy: 0.89\n",
      "[075/200] train loss: 0.042, valid loss: 0.071, f1: 0.72 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "[100/200] train loss: 0.028, valid loss: 0.068, f1: 0.73 accuracy: 0.90\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 121 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9602... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a34f85d57464a63ba7a111f627c4859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.76647880421…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>██▅▄▂▃▂▂▂▂▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▅▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>valid/loss</td><td>█▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>121</td></tr><tr><td>train_step</td><td>7563</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">stoic-planet-1017</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/5orh5r6w\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/5orh5r6w</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_003121-5orh5r6w/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1peia297\" target=\"_blank\">effortless-snowflake-1018</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 3.630, valid loss: 3.374, f1: 0.02 accuracy: 0.19\n",
      "[025/200] train loss: 0.746, valid loss: 0.771, f1: 0.43 accuracy: 0.79\n",
      "[050/200] train loss: 0.402, valid loss: 0.452, f1: 0.59 accuracy: 0.86\n",
      "[075/200] train loss: 0.295, valid loss: 0.361, f1: 0.65 accuracy: 0.89\n",
      "[100/200] train loss: 0.232, valid loss: 0.318, f1: 0.69 accuracy: 0.90\n",
      "[125/200] train loss: 0.190, valid loss: 0.295, f1: 0.73 accuracy: 0.91\n",
      "Validation loss increasing for 1 epochs\n",
      "[150/200] train loss: 0.159, valid loss: 0.282, f1: 0.75 accuracy: 0.91\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "[175/200] train loss: 0.133, valid loss: 0.275, f1: 0.75 accuracy: 0.91\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 9901... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3e923dd3284717bad44e0bc8fac316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.76399026763…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▅▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▂▃▄▄▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>valid/loss</td><td>█▆▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>train_step</td><td>12399</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">effortless-snowflake-1018</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1peia297\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/1peia297</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_003432-1peia297/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1hsmbyi1\" target=\"_blank\">sandy-totem-1019</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 3.354, valid loss: 2.998, f1: 0.02 accuracy: 0.24\n",
      "[025/200] train loss: 0.467, valid loss: 0.511, f1: 0.55 accuracy: 0.85\n",
      "[050/200] train loss: 0.279, valid loss: 0.357, f1: 0.66 accuracy: 0.89\n",
      "[075/200] train loss: 0.197, valid loss: 0.309, f1: 0.71 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[100/200] train loss: 0.141, valid loss: 0.292, f1: 0.73 accuracy: 0.91\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 122 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10347... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e316acffb94f3b989b4bd31d5078b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.76414865193…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▄▄▃▂▂▂▂▂▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▅▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▂▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>valid/loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>122</td></tr><tr><td>train_step</td><td>7625</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">sandy-totem-1019</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1hsmbyi1\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/1hsmbyi1</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_004000-1hsmbyi1/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1p0u0znp\" target=\"_blank\">glad-bee-1020</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 1.490, valid loss: 1.113, f1: 0.02 accuracy: 0.25\n",
      "[025/200] train loss: 0.105, valid loss: 0.110, f1: 0.61 accuracy: 0.86\n",
      "[050/200] train loss: 0.055, valid loss: 0.075, f1: 0.68 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[075/200] train loss: 0.034, valid loss: 0.065, f1: 0.73 accuracy: 0.91\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[100/200] train loss: 0.022, valid loss: 0.063, f1: 0.73 accuracy: 0.91\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 109 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10645... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3022a8bc4fd4d55a1ee491f5a29b273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.76386186770…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▂▄▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▅▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>valid/loss</td><td>█▆▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>109</td></tr><tr><td>train_step</td><td>6819</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">glad-bee-1020</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1p0u0znp\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/1p0u0znp</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_004323-1p0u0znp/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/2d29k0lz\" target=\"_blank\">confused-lion-1021</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 1.319, valid loss: 0.960, f1: 0.05 accuracy: 0.37\n",
      "[025/200] train loss: 0.065, valid loss: 0.082, f1: 0.68 accuracy: 0.89\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[050/200] train loss: 0.027, valid loss: 0.068, f1: 0.75 accuracy: 0.90\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 67 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 10891... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9daf991b2d3462c8251ded856ff255c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/loss</td><td>▇█▃▄▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▂▅▆▆▇▇▇▇███████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▂▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>valid/loss</td><td>█▆▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>67</td></tr><tr><td>train_step</td><td>4215</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">confused-lion-1021</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/2d29k0lz\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/2d29k0lz</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_004612-2d29k0lz/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/3bdyxw58\" target=\"_blank\">azure-yogurt-1022</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 3.276, valid loss: 2.935, f1: 0.02 accuracy: 0.24\n",
      "[025/200] train loss: 0.397, valid loss: 0.449, f1: 0.58 accuracy: 0.86\n",
      "[050/200] train loss: 0.237, valid loss: 0.330, f1: 0.67 accuracy: 0.89\n",
      "[075/200] train loss: 0.168, valid loss: 0.294, f1: 0.72 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "[100/200] train loss: 0.120, valid loss: 0.287, f1: 0.74 accuracy: 0.91\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 107 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11184... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad77ed79df5e4081a42eb2b603f87f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.76606179499…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▅▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>valid/loss</td><td>█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>107</td></tr><tr><td>train_step</td><td>6695</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">azure-yogurt-1022</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/3bdyxw58\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/3bdyxw58</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_004801-3bdyxw58/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1hkr0cn7\" target=\"_blank\">prime-surf-1023</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 3.043, valid loss: 2.694, f1: 0.05 accuracy: 0.34\n",
      "[025/200] train loss: 0.285, valid loss: 0.358, f1: 0.66 accuracy: 0.89\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[050/200] train loss: 0.149, valid loss: 0.291, f1: 0.73 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 65 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11421... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f254e752aa4e769b3ed21e0b228813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/loss</td><td>█▆▄▄▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▂▅▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>valid/loss</td><td>█▇▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>65</td></tr><tr><td>train_step</td><td>4091</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">prime-surf-1023</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1hkr0cn7\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/1hkr0cn7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_005055-1hkr0cn7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1k7movao\" target=\"_blank\">still-planet-1024</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 1.680, valid loss: 1.385, f1: 0.01 accuracy: 0.17\n",
      "[025/200] train loss: 0.265, valid loss: 0.245, f1: 0.41 accuracy: 0.77\n",
      "[050/200] train loss: 0.123, valid loss: 0.125, f1: 0.59 accuracy: 0.86\n",
      "[075/200] train loss: 0.086, valid loss: 0.095, f1: 0.63 accuracy: 0.88\n",
      "[100/200] train loss: 0.066, valid loss: 0.082, f1: 0.67 accuracy: 0.89\n",
      "[125/200] train loss: 0.052, valid loss: 0.074, f1: 0.70 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[150/200] train loss: 0.044, valid loss: 0.069, f1: 0.72 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[175/200] train loss: 0.036, valid loss: 0.066, f1: 0.74 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 11542... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a703422c2804aa0a40c7b53a4e57be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.76593137254…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▆▄▃▃▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▄▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▁▂▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>valid/loss</td><td>█▆▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>train_step</td><td>12399</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-planet-1024</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/1k7movao\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/1k7movao</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_005243-1k7movao/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/2azoapjq\" target=\"_blank\">crisp-night-1025</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 1.577, valid loss: 1.205, f1: 0.03 accuracy: 0.23\n",
      "[025/200] train loss: 0.155, valid loss: 0.149, f1: 0.54 accuracy: 0.84\n",
      "[050/200] train loss: 0.081, valid loss: 0.092, f1: 0.65 accuracy: 0.88\n",
      "[075/200] train loss: 0.055, valid loss: 0.077, f1: 0.70 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[100/200] train loss: 0.039, valid loss: 0.070, f1: 0.74 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "[125/200] train loss: 0.027, valid loss: 0.068, f1: 0.74 accuracy: 0.90\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 139 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12025... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550e3194a9114d45a26ffc7522634ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▅▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▂▄▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▁▂▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>valid/loss</td><td>█▇▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>139</td></tr><tr><td>train_step</td><td>8679</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">crisp-night-1025</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/2azoapjq\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/2azoapjq</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_005818-2azoapjq/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/rewjsmq0\" target=\"_blank\">clear-water-1026</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 3.596, valid loss: 3.401, f1: 0.01 accuracy: 0.14\n",
      "[025/200] train loss: 0.907, valid loss: 0.931, f1: 0.39 accuracy: 0.74\n",
      "[050/200] train loss: 0.482, valid loss: 0.524, f1: 0.53 accuracy: 0.85\n",
      "[075/200] train loss: 0.352, valid loss: 0.408, f1: 0.61 accuracy: 0.87\n",
      "[100/200] train loss: 0.283, valid loss: 0.354, f1: 0.65 accuracy: 0.89\n",
      "[125/200] train loss: 0.238, valid loss: 0.323, f1: 0.66 accuracy: 0.89\n",
      "[150/200] train loss: 0.204, valid loss: 0.302, f1: 0.67 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[175/200] train loss: 0.178, valid loss: 0.289, f1: 0.71 accuracy: 0.91\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12194... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9383fafa945e4857872c0d5af6d7281c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.76776900296…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▇▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▅▅▆▆▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▁▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████████</td></tr><tr><td>valid/loss</td><td>█▇▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>train_step</td><td>12399</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">clear-water-1026</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/rewjsmq0\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/rewjsmq0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_010153-rewjsmq0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/8zsfpbda\" target=\"_blank\">balmy-sunset-1027</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 3.394, valid loss: 3.058, f1: 0.01 accuracy: 0.20\n",
      "[025/200] train loss: 0.572, valid loss: 0.607, f1: 0.50 accuracy: 0.83\n",
      "[050/200] train loss: 0.330, valid loss: 0.398, f1: 0.62 accuracy: 0.88\n",
      "[075/200] train loss: 0.239, valid loss: 0.334, f1: 0.67 accuracy: 0.89\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[100/200] train loss: 0.181, valid loss: 0.305, f1: 0.72 accuracy: 0.90\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "[125/200] train loss: 0.140, valid loss: 0.292, f1: 0.74 accuracy: 0.91\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 1 epochs\n",
      "Validation loss increasing for 2 epochs\n",
      "Validation loss increasing for 3 epochs\n",
      "Validation loss increasing for 4 epochs\n",
      "Validation loss increasing for 5 epochs\n",
      "Validation loss increasing for 6 epochs\n",
      "Validation loss increasing for 7 epochs\n",
      "[150/200] train loss: 0.105, valid loss: 0.290, f1: 0.74 accuracy: 0.91\n",
      "Validation loss increasing for 8 epochs\n",
      "Validation loss increasing for 9 epochs\n",
      "Early stopping occured at epoch 153 with patience 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12270... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aaa5ffe63934c258d3123441212df3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▇▅▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▂▄▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▁▂▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>valid/loss</td><td>█▇▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>153</td></tr><tr><td>train_step</td><td>9547</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">balmy-sunset-1027</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/8zsfpbda\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/8zsfpbda</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_010734-8zsfpbda/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/kjygou5r\" target=\"_blank\">fresh-disco-1028</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 1.745, valid loss: 1.517, f1: 0.02 accuracy: 0.05\n",
      "[025/200] train loss: 0.509, valid loss: 0.444, f1: 0.28 accuracy: 0.65\n",
      "[050/200] train loss: 0.249, valid loss: 0.231, f1: 0.43 accuracy: 0.78\n",
      "[075/200] train loss: 0.166, valid loss: 0.160, f1: 0.52 accuracy: 0.84\n",
      "[100/200] train loss: 0.125, valid loss: 0.126, f1: 0.58 accuracy: 0.86\n",
      "[125/200] train loss: 0.101, valid loss: 0.108, f1: 0.62 accuracy: 0.87\n",
      "[150/200] train loss: 0.085, valid loss: 0.096, f1: 0.65 accuracy: 0.88\n",
      "[175/200] train loss: 0.073, valid loss: 0.087, f1: 0.67 accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 12334... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6722ac578f274b0aada451b203dc2ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.76790123456…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▆▄▄▁▃▂▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>valid/accuracy</td><td>▁▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>valid/f1</td><td>▁▁▂▂▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>valid/loss</td><td>█▆▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>train_step</td><td>12399</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 2 media file(s), 1 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fresh-disco-1028</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/kjygou5r\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/kjygou5r</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211216_011130-kjygou5r/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/zsg8eozu\" target=\"_blank\">bumbling-dream-1029</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n",
      "[000/200] train loss: 1.715, valid loss: 1.433, f1: 0.02 accuracy: 0.20\n",
      "[025/200] train loss: 0.293, valid loss: 0.265, f1: 0.41 accuracy: 0.76\n",
      "[050/200] train loss: 0.154, valid loss: 0.147, f1: 0.55 accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "for (tag, type, rec_size, units) in (('lstm_1L', 'lstm', 1, None), ('lstm_2L', 'lstm', 2, None),\n",
    "                                     ('fc_2L', 'lstm', 1, 128), ('gru', 'gru', 1, None)):\n",
    "    for optim in ('rmsprop', 'adam'):\n",
    "        for lr in (1e-4, 2e-4, 8e-5):\n",
    "            for fl in (True, False):\n",
    "                for hid_size in (32, 64):\n",
    "                    cfg.TYPE = type\n",
    "                    cfg.REC_SIZE = rec_size\n",
    "                    cfg.UNITS = units\n",
    "                    cfg.OPTIM = optim\n",
    "                    cfg.LR = lr\n",
    "                    cfg.HID_SIZE = hid_size\n",
    "                    _, run = train(tags=[tag], focal_loss=fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3c81pra7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8431... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>███▇▇▇▇▆▆▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>train_step</td><td>101</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">drawn-meadow-741</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/3c81pra7\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/3c81pra7</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211207_014530-3c81pra7/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3c81pra7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/2fpirjdr\" target=\"_blank\">dazzling-terrain-742</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8541... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>test/loss</td><td>0.48209</td></tr><tr><td>train_step</td><td>10199</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dazzling-terrain-742</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/2fpirjdr\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/2fpirjdr</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211207_014700-2fpirjdr/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/diego/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/l968o9d6\" target=\"_blank\">trim-smoke-743</a></strong> to <a href=\"https://wandb.ai/nlpetroni/assignment-one\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove already downloaded\n",
      "dependency_treebank already downloaded\n",
      "dependency_treebank already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TRAINING\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8612... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/loss</td><td>█▃▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>test/loss</td><td>0.32036</td></tr><tr><td>train_step</td><td>10199</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">trim-smoke-743</strong>: <a href=\"https://wandb.ai/nlpetroni/assignment-one/runs/l968o9d6\" target=\"_blank\">https://wandb.ai/nlpetroni/assignment-one/runs/l968o9d6</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211207_014804-l968o9d6/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(POSTagger(\n",
       "   (emb_layer): Embedding(400661, 100)\n",
       "   (rec_modules): LSTM(100, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "   (fc_modules): Sequential(\n",
       "     (fc_0): Linear(in_features=128, out_features=46, bias=True)\n",
       "   )\n",
       " ),\n",
       " <wandb.sdk.wandb_run.Run at 0x7f7ff4604a00>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test best model\n",
    "# first best model\n",
    "cfg.REC_SIZE = 1\n",
    "cfg.UNITS = 128\n",
    "train(test=True)\n",
    "# second best model\n",
    "cfg.REC_SIZE = 2\n",
    "cfg.UNITS = None\n",
    "train(test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d92e98c3664063ec1b567951c01aa42f8ffade76e6df5a130cb26ea124003d56"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
